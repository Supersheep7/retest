{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c9d28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the notebook with beliefs_llms env for torch\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "PATH = Path('full_results')\n",
    "models = ['gpt-j', 'llama', 'llama_instruct', 'gemma', 'gemma_instruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e9ec9",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad77e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============  llama  ===============\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "['0.497', '0.512', '0.515', '0.543', '0.562', '0.599', '0.651', '0.716', '0.762', '0.787', '0.827', '0.885', '0.899', '0.903', '0.898', '0.899', '0.901', '0.894', '0.889', '0.885', '0.881', '0.887', '0.885', '0.888', '0.885', '0.883', '0.876', '0.871', '0.868', '0.860', '0.868', '0.858']\n",
      "\n",
      "Score:  0.9001723147616312\n",
      "\n",
      "HEADS\n",
      "\n",
      "['0.493', '0.513', '0.512', '0.509', '0.503', '0.497', '0.507', '0.503', '0.519', '0.501', '0.520', '0.517', '0.499', '0.509', '0.504', '0.505', '0.497', '0.492', '0.499', '0.498', '0.503', '0.509', '0.498', '0.501', '0.503', '0.510', '0.501', '0.499', '0.528', '0.491', '0.504', '0.499']\n",
      "['0.516', '0.514', '0.515', '0.507', '0.496', '0.498', '0.501', '0.507', '0.518', '0.512', '0.504', '0.501', '0.507', '0.498', '0.524', '0.507', '0.510', '0.487', '0.510', '0.501', '0.500', '0.495', '0.512', '0.511', '0.494', '0.500', '0.496', '0.500', '0.498', '0.513', '0.507', '0.515']\n",
      "['0.494', '0.508', '0.504', '0.491', '0.525', '0.523', '0.523', '0.508', '0.480', '0.482', '0.487', '0.474', '0.499', '0.501', '0.515', '0.504', '0.495', '0.501', '0.500', '0.542', '0.499', '0.515', '0.511', '0.507', '0.498', '0.492', '0.520', '0.499', '0.497', '0.508', '0.506', '0.507']\n",
      "['0.529', '0.513', '0.517', '0.506', '0.514', '0.488', '0.521', '0.505', '0.495', '0.493', '0.497', '0.506', '0.538', '0.519', '0.528', '0.512', '0.497', '0.502', '0.490', '0.518', '0.531', '0.531', '0.527', '0.530', '0.542', '0.543', '0.524', '0.513', '0.494', '0.559', '0.520', '0.532']\n",
      "['0.538', '0.567', '0.583', '0.522', '0.502', '0.512', '0.520', '0.502', '0.579', '0.557', '0.559', '0.517', '0.523', '0.527', '0.540', '0.573', '0.527', '0.512', '0.542', '0.524', '0.537', '0.509', '0.508', '0.532', '0.520', '0.513', '0.511', '0.523', '0.524', '0.511', '0.522', '0.511']\n",
      "['0.528', '0.536', '0.513', '0.508', '0.527', '0.600', '0.570', '0.584', '0.540', '0.544', '0.543', '0.538', '0.534', '0.511', '0.509', '0.518', '0.590', '0.577', '0.557', '0.540', '0.542', '0.543', '0.511', '0.524', '0.578', '0.555', '0.574', '0.582', '0.518', '0.520', '0.512', '0.547']\n",
      "['0.550', '0.531', '0.577', '0.612', '0.541', '0.582', '0.601', '0.555', '0.523', '0.567', '0.520', '0.513', '0.587', '0.541', '0.630', '0.558', '0.520', '0.569', '0.531', '0.558', '0.584', '0.576', '0.559', '0.585', '0.566', '0.570', '0.549', '0.556', '0.573', '0.617', '0.584', '0.536']\n",
      "['0.532', '0.536', '0.538', '0.546', '0.592', '0.551', '0.639', '0.634', '0.553', '0.557', '0.574', '0.557', '0.633', '0.649', '0.621', '0.661', '0.569', '0.549', '0.581', '0.558', '0.617', '0.620', '0.589', '0.555', '0.592', '0.551', '0.547', '0.565', '0.641', '0.633', '0.651', '0.653']\n",
      "['0.601', '0.583', '0.572', '0.582', '0.605', '0.611', '0.637', '0.677', '0.655', '0.618', '0.685', '0.567', '0.679', '0.586', '0.690', '0.631', '0.593', '0.604', '0.580', '0.579', '0.584', '0.613', '0.571', '0.571', '0.633', '0.659', '0.692', '0.681', '0.578', '0.642', '0.608', '0.588']\n",
      "['0.650', '0.673', '0.597', '0.668', '0.661', '0.709', '0.724', '0.759', '0.565', '0.561', '0.579', '0.569', '0.643', '0.589', '0.627', '0.672', '0.674', '0.712', '0.657', '0.684', '0.675', '0.722', '0.677', '0.735', '0.665', '0.595', '0.591', '0.585', '0.627', '0.635', '0.639', '0.592']\n",
      "['0.623', '0.627', '0.610', '0.620', '0.713', '0.617', '0.693', '0.610', '0.630', '0.647', '0.678', '0.600', '0.570', '0.571', '0.558', '0.670', '0.585', '0.685', '0.595', '0.627', '0.708', '0.770', '0.700', '0.774', '0.759', '0.702', '0.763', '0.740', '0.752', '0.689', '0.603', '0.628']\n",
      "['0.760', '0.796', '0.741', '0.788', '0.773', '0.641', '0.597', '0.720', '0.716', '0.804', '0.686', '0.802', '0.782', '0.827', '0.830', '0.814', '0.617', '0.652', '0.768', '0.670', '0.778', '0.785', '0.728', '0.658', '0.778', '0.778', '0.796', '0.769', '0.609', '0.726', '0.656', '0.688']\n",
      "['0.755', '0.794', '0.701', '0.722', '0.783', '0.829', '0.823', '0.772', '0.811', '0.747', '0.789', '0.796', '0.671', '0.750', '0.827', '0.804', '0.648', '0.686', '0.691', '0.657', '0.782', '0.829', '0.805', '0.737', '0.709', '0.743', '0.696', '0.777', '0.769', '0.716', '0.717', '0.804']\n",
      "['0.790', '0.737', '0.798', '0.737', '0.701', '0.808', '0.675', '0.812', '0.704', '0.791', '0.820', '0.805', '0.816', '0.755', '0.818', '0.815', '0.773', '0.836', '0.769', '0.780', '0.829', '0.864', '0.805', '0.858', '0.792', '0.806', '0.748', '0.739', '0.792', '0.826', '0.842', '0.691']\n",
      "['0.797', '0.790', '0.820', '0.809', '0.831', '0.790', '0.793', '0.842', '0.714', '0.783', '0.798', '0.758', '0.657', '0.674', '0.763', '0.643', '0.674', '0.622', '0.678', '0.613', '0.827', '0.794', '0.810', '0.838', '0.832', '0.745', '0.696', '0.862', '0.716', '0.715', '0.826', '0.756']\n",
      "['0.704', '0.692', '0.696', '0.725', '0.789', '0.818', '0.793', '0.757', '0.767', '0.795', '0.782', '0.794', '0.681', '0.745', '0.690', '0.765', '0.766', '0.706', '0.813', '0.855', '0.769', '0.701', '0.755', '0.700', '0.754', '0.702', '0.709', '0.736', '0.659', '0.668', '0.668', '0.698']\n",
      "['0.754', '0.708', '0.678', '0.667', '0.791', '0.751', '0.817', '0.800', '0.681', '0.680', '0.695', '0.742', '0.735', '0.804', '0.774', '0.778', '0.789', '0.817', '0.763', '0.781', '0.658', '0.662', '0.644', '0.715', '0.775', '0.764', '0.812', '0.672', '0.821', '0.736', '0.729', '0.836']\n",
      "['0.670', '0.743', '0.693', '0.693', '0.789', '0.777', '0.800', '0.781', '0.712', '0.721', '0.814', '0.751', '0.775', '0.663', '0.762', '0.712', '0.762', '0.807', '0.780', '0.787', '0.794', '0.761', '0.810', '0.701', '0.798', '0.808', '0.759', '0.814', '0.713', '0.669', '0.778', '0.798']\n",
      "['0.763', '0.760', '0.682', '0.812', '0.796', '0.633', '0.717', '0.698', '0.729', '0.713', '0.800', '0.777', '0.731', '0.707', '0.655', '0.701', '0.783', '0.656', '0.616', '0.789', '0.815', '0.675', '0.808', '0.799', '0.752', '0.790', '0.751', '0.787', '0.752', '0.717', '0.635', '0.773']\n",
      "['0.655', '0.643', '0.621', '0.698', '0.767', '0.803', '0.779', '0.759', '0.717', '0.698', '0.761', '0.766', '0.654', '0.661', '0.668', '0.752', '0.830', '0.813', '0.840', '0.845', '0.628', '0.689', '0.786', '0.690', '0.634', '0.600', '0.667', '0.593', '0.697', '0.739', '0.763', '0.759']\n",
      "['0.728', '0.624', '0.764', '0.591', '0.702', '0.754', '0.773', '0.753', '0.777', '0.682', '0.702', '0.822', '0.778', '0.647', '0.648', '0.744', '0.790', '0.782', '0.708', '0.744', '0.627', '0.673', '0.741', '0.642', '0.743', '0.742', '0.758', '0.754', '0.803', '0.739', '0.802', '0.789']\n",
      "['0.757', '0.707', '0.694', '0.702', '0.709', '0.777', '0.752', '0.678', '0.702', '0.708', '0.675', '0.754', '0.685', '0.659', '0.605', '0.685', '0.775', '0.787', '0.770', '0.800', '0.730', '0.704', '0.661', '0.777', '0.736', '0.729', '0.655', '0.813', '0.614', '0.657', '0.608', '0.649']\n",
      "['0.629', '0.666', '0.668', '0.717', '0.783', '0.811', '0.778', '0.786', '0.691', '0.650', '0.607', '0.684', '0.726', '0.706', '0.643', '0.720', '0.723', '0.774', '0.752', '0.718', '0.745', '0.762', '0.681', '0.702', '0.743', '0.755', '0.727', '0.666', '0.689', '0.717', '0.735', '0.841']\n",
      "['0.756', '0.753', '0.742', '0.717', '0.779', '0.739', '0.666', '0.769', '0.626', '0.644', '0.665', '0.680', '0.701', '0.700', '0.654', '0.679', '0.692', '0.710', '0.638', '0.750', '0.621', '0.734', '0.607', '0.636', '0.678', '0.665', '0.738', '0.666', '0.826', '0.720', '0.737', '0.738']\n",
      "['0.654', '0.704', '0.652', '0.667', '0.785', '0.766', '0.761', '0.791', '0.619', '0.659', '0.688', '0.654', '0.783', '0.791', '0.768', '0.717', '0.621', '0.593', '0.619', '0.715', '0.662', '0.674', '0.647', '0.657', '0.671', '0.633', '0.705', '0.621', '0.735', '0.733', '0.719', '0.708']\n",
      "['0.773', '0.806', '0.783', '0.753', '0.647', '0.658', '0.643', '0.657', '0.750', '0.747', '0.713', '0.760', '0.675', '0.654', '0.658', '0.642', '0.738', '0.681', '0.685', '0.786', '0.674', '0.685', '0.689', '0.679', '0.743', '0.670', '0.736', '0.671', '0.708', '0.717', '0.692', '0.695']\n",
      "['0.644', '0.678', '0.654', '0.670', '0.696', '0.682', '0.716', '0.700', '0.697', '0.702', '0.683', '0.697', '0.671', '0.610', '0.623', '0.639', '0.733', '0.765', '0.740', '0.697', '0.720', '0.782', '0.773', '0.768', '0.690', '0.651', '0.716', '0.642', '0.733', '0.617', '0.630', '0.660']\n",
      "['0.645', '0.709', '0.646', '0.633', '0.660', '0.682', '0.661', '0.672', '0.704', '0.725', '0.742', '0.710', '0.706', '0.694', '0.702', '0.701', '0.676', '0.748', '0.684', '0.682', '0.588', '0.651', '0.602', '0.599', '0.759', '0.646', '0.729', '0.694', '0.689', '0.688', '0.702', '0.708']\n",
      "['0.612', '0.654', '0.635', '0.691', '0.668', '0.635', '0.646', '0.681', '0.738', '0.700', '0.689', '0.712', '0.662', '0.639', '0.677', '0.616', '0.741', '0.650', '0.650', '0.714', '0.677', '0.688', '0.702', '0.678', '0.723', '0.686', '0.639', '0.693', '0.729', '0.669', '0.661', '0.712']\n",
      "['0.743', '0.672', '0.638', '0.700', '0.639', '0.686', '0.624', '0.635', '0.631', '0.711', '0.666', '0.665', '0.677', '0.652', '0.663', '0.674', '0.685', '0.718', '0.739', '0.710', '0.658', '0.686', '0.659', '0.715', '0.643', '0.659', '0.592', '0.646', '0.626', '0.694', '0.631', '0.635']\n",
      "['0.659', '0.671', '0.651', '0.671', '0.647', '0.655', '0.685', '0.721', '0.732', '0.659', '0.654', '0.604', '0.670', '0.751', '0.666', '0.662', '0.713', '0.731', '0.698', '0.737', '0.709', '0.616', '0.723', '0.682', '0.686', '0.676', '0.695', '0.684', '0.743', '0.663', '0.652', '0.748']\n",
      "['0.679', '0.709', '0.748', '0.733', '0.678', '0.695', '0.665', '0.652', '0.733', '0.685', '0.744', '0.665', '0.677', '0.654', '0.671', '0.647', '0.666', '0.736', '0.674', '0.801', '0.634', '0.622', '0.686', '0.594', '0.706', '0.675', '0.770', '0.727', '0.739', '0.760', '0.664', '0.756']\n",
      "\n",
      "Score:  0.8567489948305571\n",
      "\n",
      "===============  llama_instruct  ===============\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "['0.551', '0.568', '0.577', '0.595', '0.615', '0.617', '0.655', '0.690', '0.723', '0.771', '0.813', '0.835', '0.868', '0.881', '0.881', '0.883', '0.881', '0.874', '0.875', '0.874', '0.865', '0.878', '0.877', '0.883', '0.881', '0.878', '0.881', '0.874', '0.871', '0.866', '0.870', '0.878']\n",
      "\n",
      "Score:  0.8819069500287192\n",
      "\n",
      "HEADS\n",
      "\n",
      "['0.507', '0.508', '0.506', '0.501', '0.513', '0.506', '0.515', '0.512', '0.518', '0.511', '0.511', '0.508', '0.497', '0.497', '0.499', '0.497', '0.499', '0.501', '0.504', '0.501', '0.496', '0.493', '0.499', '0.499', '0.487', '0.488', '0.494', '0.485', '0.525', '0.513', '0.523', '0.511']\n",
      "['0.507', '0.494', '0.527', '0.506', '0.517', '0.512', '0.516', '0.531', '0.511', '0.517', '0.518', '0.504', '0.502', '0.501', '0.499', '0.501', '0.489', '0.489', '0.489', '0.503', '0.509', '0.505', '0.512', '0.511', '0.491', '0.496', '0.493', '0.494', '0.522', '0.515', '0.506', '0.508']\n",
      "['0.504', '0.512', '0.496', '0.489', '0.523', '0.538', '0.530', '0.527', '0.523', '0.526', '0.522', '0.519', '0.492', '0.505', '0.489', '0.484', '0.520', '0.524', '0.515', '0.515', '0.508', '0.511', '0.497', '0.495', '0.516', '0.499', '0.513', '0.508', '0.520', '0.522', '0.516', '0.520']\n",
      "['0.548', '0.540', '0.523', '0.530', '0.532', '0.528', '0.528', '0.519', '0.520', '0.509', '0.513', '0.520', '0.533', '0.515', '0.530', '0.527', '0.515', '0.510', '0.514', '0.514', '0.522', '0.535', '0.540', '0.539', '0.522', '0.520', '0.532', '0.524', '0.512', '0.555', '0.534', '0.542']\n",
      "['0.561', '0.576', '0.575', '0.576', '0.531', '0.527', '0.534', '0.511', '0.553', '0.543', '0.561', '0.540', '0.551', '0.538', '0.528', '0.574', '0.545', '0.535', '0.534', '0.553', '0.526', '0.534', '0.537', '0.536', '0.523', '0.524', '0.525', '0.522', '0.543', '0.538', '0.531', '0.511']\n",
      "['0.548', '0.547', '0.540', '0.540', '0.553', '0.573', '0.573', '0.577', '0.527', '0.532', '0.530', '0.527', '0.563', '0.565', '0.551', '0.543', '0.532', '0.551', '0.535', '0.523', '0.555', '0.543', '0.553', '0.563', '0.537', '0.543', '0.519', '0.565', '0.551', '0.549', '0.552', '0.536']\n",
      "['0.575', '0.580', '0.564', '0.574', '0.553', '0.551', '0.593', '0.565', '0.522', '0.576', '0.526', '0.510', '0.541', '0.550', '0.635', '0.533', '0.532', '0.545', '0.527', '0.545', '0.603', '0.538', '0.546', '0.547', '0.584', '0.545', '0.571', '0.570', '0.562', '0.597', '0.600', '0.557']\n",
      "['0.530', '0.513', '0.532', '0.549', '0.572', '0.513', '0.620', '0.606', '0.547', '0.599', '0.570', '0.524', '0.562', '0.572', '0.589', '0.607', '0.581', '0.578', '0.610', '0.556', '0.609', '0.613', '0.576', '0.555', '0.605', '0.573', '0.589', '0.577', '0.594', '0.621', '0.573', '0.594']\n",
      "['0.595', '0.551', '0.580', '0.550', '0.580', '0.566', '0.579', '0.695', '0.620', '0.613', '0.668', '0.569', '0.642', '0.589', '0.660', '0.599', '0.580', '0.611', '0.578', '0.579', '0.588', '0.592', '0.572', '0.563', '0.601', '0.646', '0.628', '0.654', '0.623', '0.623', '0.618', '0.598']\n",
      "['0.648', '0.696', '0.647', '0.616', '0.690', '0.661', '0.726', '0.782', '0.599', '0.580', '0.567', '0.570', '0.642', '0.596', '0.688', '0.593', '0.679', '0.690', '0.628', '0.692', '0.648', '0.717', '0.654', '0.674', '0.689', '0.559', '0.602', '0.641', '0.605', '0.666', '0.678', '0.638']\n",
      "['0.628', '0.646', '0.617', '0.693', '0.643', '0.640', '0.636', '0.588', '0.648', '0.630', '0.624', '0.590', '0.555', '0.589', '0.568', '0.648', '0.710', '0.717', '0.700', '0.648', '0.758', '0.771', '0.654', '0.805', '0.832', '0.705', '0.721', '0.803', '0.716', '0.757', '0.669', '0.669']\n",
      "['0.701', '0.796', '0.746', '0.779', '0.759', '0.679', '0.655', '0.696', '0.674', '0.798', '0.655', '0.805', '0.698', '0.820', '0.815', '0.789', '0.612', '0.739', '0.746', '0.712', '0.762', '0.716', '0.677', '0.637', '0.833', '0.834', '0.844', '0.829', '0.612', '0.748', '0.632', '0.750']\n",
      "['0.673', '0.805', '0.665', '0.665', '0.766', '0.827', '0.798', '0.760', '0.785', '0.782', '0.713', '0.729', '0.647', '0.698', '0.740', '0.703', '0.635', '0.745', '0.722', '0.669', '0.724', '0.808', '0.762', '0.678', '0.658', '0.658', '0.650', '0.658', '0.682', '0.670', '0.652', '0.726']\n",
      "['0.853', '0.835', '0.786', '0.775', '0.875', '0.872', '0.812', '0.835', '0.735', '0.820', '0.829', '0.821', '0.896', '0.779', '0.814', '0.844', '0.747', '0.868', '0.813', '0.717', '0.750', '0.841', '0.740', '0.751', '0.746', '0.721', '0.748', '0.712', '0.755', '0.740', '0.875', '0.788']\n",
      "['0.873', '0.847', '0.858', '0.871', '0.821', '0.862', '0.804', '0.841', '0.715', '0.779', '0.796', '0.725', '0.786', '0.787', '0.802', '0.765', '0.679', '0.792', '0.762', '0.702', '0.858', '0.851', '0.872', '0.855', '0.889', '0.856', '0.672', '0.844', '0.774', '0.782', '0.877', '0.846']\n",
      "['0.821', '0.710', '0.733', '0.864', '0.882', '0.835', '0.820', '0.812', '0.827', '0.837', '0.808', '0.867', '0.779', '0.814', '0.812', '0.834', '0.804', '0.775', '0.829', '0.847', '0.834', '0.821', '0.816', '0.845', '0.810', '0.800', '0.810', '0.830', '0.748', '0.770', '0.690', '0.820']\n",
      "['0.833', '0.802', '0.747', '0.772', '0.843', '0.823', '0.824', '0.833', '0.845', '0.820', '0.857', '0.829', '0.751', '0.817', '0.785', '0.824', '0.821', '0.822', '0.848', '0.858', '0.756', '0.785', '0.787', '0.831', '0.851', '0.862', '0.865', '0.841', '0.868', '0.794', '0.791', '0.853']\n",
      "['0.833', '0.820', '0.835', '0.840', '0.859', '0.841', '0.848', '0.854', '0.809', '0.864', '0.846', '0.851', '0.774', '0.832', '0.833', '0.823', '0.829', '0.845', '0.851', '0.859', '0.902', '0.902', '0.872', '0.862', '0.841', '0.842', '0.811', '0.821', '0.858', '0.843', '0.844', '0.881']\n",
      "['0.903', '0.872', '0.866', '0.874', '0.881', '0.819', '0.844', '0.800', '0.870', '0.865', '0.877', '0.891', '0.855', '0.839', '0.845', '0.851', '0.848', '0.723', '0.795', '0.855', '0.871', '0.797', '0.830', '0.816', '0.874', '0.877', '0.837', '0.873', '0.826', '0.821', '0.801', '0.820']\n",
      "['0.821', '0.832', '0.823', '0.833', '0.854', '0.855', '0.848', '0.851', '0.854', '0.862', '0.865', '0.834', '0.769', '0.804', '0.833', '0.820', '0.841', '0.831', '0.846', '0.867', '0.767', '0.847', '0.823', '0.825', '0.817', '0.831', '0.773', '0.841', '0.827', '0.827', '0.830', '0.821']\n",
      "['0.787', '0.814', '0.837', '0.813', '0.813', '0.815', '0.848', '0.827', '0.871', '0.865', '0.860', '0.851', '0.854', '0.821', '0.820', '0.827', '0.834', '0.828', '0.813', '0.824', '0.795', '0.781', '0.851', '0.844', '0.865', '0.834', '0.866', '0.835', '0.890', '0.855', '0.879', '0.878']\n",
      "['0.855', '0.824', '0.806', '0.755', '0.809', '0.851', '0.838', '0.779', '0.848', '0.838', '0.788', '0.871', '0.820', '0.817', '0.816', '0.828', '0.821', '0.814', '0.819', '0.835', '0.831', '0.806', '0.789', '0.796', '0.843', '0.850', '0.855', '0.833', '0.809', '0.818', '0.844', '0.816']\n",
      "['0.814', '0.809', '0.841', '0.834', '0.814', '0.821', '0.817', '0.822', '0.839', '0.808', '0.843', '0.840', '0.812', '0.828', '0.811', '0.801', '0.835', '0.825', '0.802', '0.833', '0.845', '0.845', '0.782', '0.855', '0.846', '0.804', '0.815', '0.830', '0.829', '0.812', '0.809', '0.864']\n",
      "['0.779', '0.790', '0.774', '0.792', '0.804', '0.787', '0.812', '0.805', '0.813', '0.820', '0.817', '0.804', '0.834', '0.851', '0.811', '0.830', '0.817', '0.833', '0.824', '0.858', '0.820', '0.814', '0.789', '0.770', '0.808', '0.833', '0.819', '0.825', '0.887', '0.823', '0.849', '0.849']\n",
      "['0.812', '0.817', '0.802', '0.828', '0.822', '0.827', '0.819', '0.820', '0.783', '0.761', '0.789', '0.781', '0.790', '0.780', '0.790', '0.759', '0.830', '0.828', '0.824', '0.837', '0.807', '0.814', '0.804', '0.829', '0.797', '0.804', '0.823', '0.801', '0.816', '0.810', '0.808', '0.813']\n",
      "['0.824', '0.814', '0.823', '0.817', '0.803', '0.813', '0.825', '0.810', '0.820', '0.816', '0.819', '0.842', '0.800', '0.795', '0.780', '0.797', '0.852', '0.848', '0.883', '0.849', '0.740', '0.762', '0.740', '0.785', '0.822', '0.849', '0.846', '0.860', '0.779', '0.758', '0.796', '0.783']\n",
      "['0.790', '0.776', '0.805', '0.808', '0.808', '0.826', '0.827', '0.812', '0.791', '0.797', '0.836', '0.798', '0.814', '0.805', '0.803', '0.804', '0.840', '0.858', '0.833', '0.854', '0.789', '0.855', '0.790', '0.847', '0.806', '0.812', '0.856', '0.822', '0.829', '0.773', '0.828', '0.786']\n",
      "['0.787', '0.808', '0.806', '0.806', '0.804', '0.768', '0.820', '0.794', '0.844', '0.814', '0.848', '0.787', '0.791', '0.809', '0.800', '0.801', '0.819', '0.802', '0.804', '0.796', '0.785', '0.831', '0.812', '0.812', '0.815', '0.769', '0.783', '0.773', '0.823', '0.762', '0.778', '0.821']\n",
      "['0.790', '0.798', '0.783', '0.755', '0.751', '0.759', '0.771', '0.768', '0.833', '0.812', '0.878', '0.775', '0.816', '0.804', '0.814', '0.815', '0.812', '0.786', '0.825', '0.816', '0.792', '0.741', '0.765', '0.790', '0.809', '0.809', '0.723', '0.779', '0.810', '0.768', '0.813', '0.796']\n",
      "['0.838', '0.828', '0.812', '0.817', '0.820', '0.835', '0.821', '0.818', '0.836', '0.805', '0.786', '0.798', '0.788', '0.812', '0.802', '0.795', '0.817', '0.822', '0.823', '0.806', '0.825', '0.823', '0.826', '0.825', '0.784', '0.794', '0.763', '0.781', '0.818', '0.816', '0.834', '0.813']\n",
      "['0.774', '0.844', '0.830', '0.824', '0.802', '0.785', '0.825', '0.798', '0.814', '0.826', '0.791', '0.793', '0.767', '0.815', '0.674', '0.747', '0.824', '0.820', '0.821', '0.834', '0.809', '0.738', '0.797', '0.789', '0.798', '0.777', '0.815', '0.831', '0.827', '0.785', '0.783', '0.826']\n",
      "['0.840', '0.830', '0.802', '0.824', '0.786', '0.763', '0.794', '0.778', '0.790', '0.787', '0.754', '0.793', '0.777', '0.787', '0.779', '0.772', '0.787', '0.814', '0.784', '0.832', '0.770', '0.755', '0.804', '0.758', '0.809', '0.803', '0.833', '0.806', '0.843', '0.805', '0.771', '0.792']\n",
      "\n",
      "Score:  0.8987937966685813\n",
      "\n",
      "===============  gpt-j  ===============\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "['0.491', '0.489', '0.508', '0.532', '0.531', '0.578', '0.600', '0.626', '0.688', '0.734', '0.770', '0.773', '0.776', '0.782', '0.800', '0.796', '0.800', '0.802', '0.795', '0.787', '0.786', '0.794', '0.804', '0.800', '0.798', '0.798', '0.797', '0.794']\n",
      "\n",
      "Score:  0.8010338885697875\n",
      "\n",
      "HEADS\n",
      "\n",
      "['0.489', '0.505', '0.490', '0.490', '0.516', '0.497', '0.512', '0.485', '0.507', '0.501', '0.516', '0.519', '0.492', '0.499', '0.534', '0.481']\n",
      "['0.500', '0.488', '0.495', '0.501', '0.492', '0.512', '0.495', '0.504', '0.499', '0.500', '0.494', '0.500', '0.499', '0.506', '0.513', '0.507']\n",
      "['0.520', '0.511', '0.516', '0.501', '0.522', '0.515', '0.495', '0.495', '0.531', '0.512', '0.515', '0.515', '0.518', '0.509', '0.499', '0.510']\n",
      "['0.528', '0.523', '0.512', '0.530', '0.513', '0.506', '0.510', '0.506', '0.518', '0.507', '0.505', '0.515', '0.499', '0.512', '0.507', '0.504']\n",
      "['0.530', '0.504', '0.516', '0.508', '0.562', '0.518', '0.520', '0.521', '0.549', '0.513', '0.529', '0.538', '0.517', '0.522', '0.522', '0.531']\n",
      "['0.565', '0.567', '0.556', '0.541', '0.557', '0.562', '0.568', '0.572', '0.561', '0.560', '0.532', '0.558', '0.573', '0.557', '0.567', '0.536']\n",
      "['0.567', '0.585', '0.590', '0.572', '0.577', '0.562', '0.580', '0.561', '0.550', '0.573', '0.578', '0.584', '0.569', '0.586', '0.583', '0.597']\n",
      "['0.616', '0.590', '0.589', '0.559', '0.597', '0.591', '0.592', '0.592', '0.594', '0.599', '0.595', '0.599', '0.583', '0.591', '0.607', '0.573']\n",
      "['0.640', '0.631', '0.612', '0.627', '0.624', '0.637', '0.658', '0.631', '0.624', '0.631', '0.630', '0.631', '0.619', '0.635', '0.626', '0.615']\n",
      "['0.660', '0.667', '0.636', '0.656', '0.612', '0.639', '0.642', '0.636', '0.630', '0.635', '0.651', '0.709', '0.662', '0.649', '0.671', '0.646']\n",
      "['0.695', '0.701', '0.690', '0.667', '0.705', '0.720', '0.705', '0.698', '0.671', '0.684', '0.706', '0.706', '0.688', '0.686', '0.688', '0.667']\n",
      "['0.688', '0.717', '0.703', '0.659', '0.726', '0.687', '0.663', '0.680', '0.666', '0.690', '0.709', '0.681', '0.712', '0.700', '0.677', '0.689']\n",
      "['0.665', '0.686', '0.690', '0.661', '0.686', '0.705', '0.691', '0.643', '0.669', '0.675', '0.667', '0.667', '0.637', '0.712', '0.698', '0.667']\n",
      "['0.695', '0.716', '0.638', '0.669', '0.659', '0.708', '0.665', '0.682', '0.697', '0.672', '0.699', '0.687', '0.702', '0.705', '0.706', '0.684']\n",
      "['0.690', '0.703', '0.663', '0.665', '0.705', '0.684', '0.635', '0.671', '0.680', '0.682', '0.665', '0.684', '0.694', '0.653', '0.679', '0.701']\n",
      "['0.655', '0.651', '0.664', '0.701', '0.650', '0.693', '0.670', '0.684', '0.705', '0.678', '0.662', '0.680', '0.662', '0.672', '0.698', '0.662']\n",
      "['0.634', '0.654', '0.682', '0.710', '0.681', '0.662', '0.675', '0.624', '0.666', '0.651', '0.624', '0.633', '0.670', '0.671', '0.705', '0.633']\n",
      "['0.655', '0.650', '0.655', '0.639', '0.668', '0.677', '0.620', '0.675', '0.659', '0.643', '0.667', '0.643', '0.648', '0.671', '0.663', '0.659']\n",
      "['0.667', '0.673', '0.638', '0.654', '0.631', '0.590', '0.685', '0.641', '0.664', '0.678', '0.677', '0.693', '0.647', '0.633', '0.635', '0.673']\n",
      "['0.648', '0.630', '0.694', '0.651', '0.639', '0.686', '0.620', '0.624', '0.639', '0.642', '0.641', '0.644', '0.617', '0.615', '0.615', '0.615']\n",
      "['0.630', '0.624', '0.614', '0.686', '0.631', '0.636', '0.590', '0.652', '0.597', '0.636', '0.660', '0.651', '0.678', '0.654', '0.658', '0.654']\n",
      "['0.640', '0.709', '0.615', '0.626', '0.605', '0.657', '0.625', '0.640', '0.618', '0.640', '0.618', '0.632', '0.608', '0.604', '0.599', '0.608']\n",
      "['0.690', '0.626', '0.641', '0.620', '0.593', '0.601', '0.657', '0.611', '0.616', '0.635', '0.618', '0.586', '0.626', '0.626', '0.597', '0.626']\n",
      "['0.640', '0.630', '0.599', '0.638', '0.579', '0.625', '0.638', '0.620', '0.683', '0.608', '0.636', '0.603', '0.631', '0.598', '0.572', '0.604']\n",
      "['0.622', '0.650', '0.626', '0.625', '0.634', '0.621', '0.593', '0.620', '0.655', '0.650', '0.598', '0.598', '0.634', '0.669', '0.605', '0.604']\n",
      "['0.640', '0.640', '0.605', '0.604', '0.667', '0.639', '0.611', '0.611', '0.633', '0.617', '0.627', '0.610', '0.615', '0.592', '0.615', '0.660']\n",
      "['0.630', '0.628', '0.641', '0.609', '0.626', '0.628', '0.608', '0.631', '0.614', '0.603', '0.661', '0.590', '0.638', '0.603', '0.601', '0.606']\n",
      "['0.646', '0.616', '0.619', '0.650', '0.632', '0.646', '0.645', '0.605', '0.604', '0.651', '0.624', '0.612', '0.638', '0.601', '0.626', '0.611']\n",
      "\n",
      "Score:  0.7182079264790351\n",
      "\n",
      "===============  gemma  ===============\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "['0.517', '0.501', '0.507', '0.495', '0.550', '0.566', '0.595', '0.600', '0.637', '0.671', '0.677', '0.679', '0.684', '0.712', '0.737', '0.769', '0.799', '0.852', '0.857', '0.891', '0.908', '0.907', '0.913', '0.917', '0.911', '0.922', '0.922', '0.922', '0.915', '0.917', '0.909', '0.908', '0.908', '0.903', '0.900', '0.901', '0.901', '0.898', '0.898', '0.901', '0.893', '0.890']\n",
      "\n",
      "Score:  0.9200459506031017\n",
      "\n",
      "HEADS\n",
      "\n",
      "['0.519', '0.493', '0.504', '0.505', '0.496', '0.527', '0.518', '0.501', '0.503', '0.504', '0.527', '0.515', '0.526', '0.489', '0.492', '0.524']\n",
      "['0.511', '0.513', '0.512', '0.501', '0.512', '0.501', '0.500', '0.521', '0.491', '0.488', '0.515', '0.514', '0.501', '0.500', '0.531', '0.519']\n",
      "['0.501', '0.529', '0.525', '0.514', '0.519', '0.507', '0.505', '0.518', '0.513', '0.513', '0.506', '0.505', '0.529', '0.512', '0.507', '0.512']\n",
      "['0.500', '0.515', '0.507', '0.514', '0.499', '0.497', '0.492', '0.519', '0.520', '0.503', '0.522', '0.503', '0.497', '0.513', '0.514', '0.507']\n",
      "['0.514', '0.508', '0.531', '0.534', '0.545', '0.531', '0.545', '0.536', '0.501', '0.511', '0.543', '0.530', '0.512', '0.531', '0.505', '0.538']\n",
      "['0.543', '0.527', '0.530', '0.540', '0.548', '0.549', '0.532', '0.523', '0.548', '0.547', '0.540', '0.542', '0.547', '0.538', '0.545', '0.536']\n",
      "['0.526', '0.535', '0.554', '0.581', '0.562', '0.542', '0.587', '0.585', '0.543', '0.532', '0.582', '0.579', '0.545', '0.558', '0.550', '0.551']\n",
      "['0.558', '0.557', '0.608', '0.590', '0.571', '0.569', '0.567', '0.565', '0.557', '0.563', '0.558', '0.556', '0.567', '0.596', '0.589', '0.592']\n",
      "['0.591', '0.570', '0.552', '0.594', '0.589', '0.562', '0.573', '0.612', '0.599', '0.567', '0.613', '0.635', '0.567', '0.575', '0.638', '0.624']\n",
      "['0.618', '0.603', '0.613', '0.595', '0.584', '0.577', '0.644', '0.636', '0.602', '0.584', '0.572', '0.636', '0.644', '0.596', '0.595', '0.598']\n",
      "['0.602', '0.607', '0.635', '0.664', '0.569', '0.584', '0.639', '0.594', '0.572', '0.593', '0.616', '0.624', '0.624', '0.620', '0.585', '0.589']\n",
      "['0.572', '0.589', '0.603', '0.600', '0.612', '0.614', '0.627', '0.621', '0.655', '0.638', '0.646', '0.615', '0.573', '0.635', '0.653', '0.673']\n",
      "['0.640', '0.592', '0.585', '0.696', '0.582', '0.597', '0.590', '0.600', '0.594', '0.592', '0.599', '0.582', '0.609', '0.650', '0.608', '0.592']\n",
      "['0.685', '0.649', '0.601', '0.609', '0.623', '0.588', '0.611', '0.608', '0.601', '0.599', '0.723', '0.664', '0.686', '0.689', '0.606', '0.599']\n",
      "['0.706', '0.762', '0.606', '0.614', '0.610', '0.607', '0.599', '0.621', '0.604', '0.626', '0.590', '0.592', '0.655', '0.610', '0.744', '0.766']\n",
      "['0.651', '0.628', '0.598', '0.593', '0.701', '0.666', '0.610', '0.674', '0.613', '0.617', '0.622', '0.749', '0.607', '0.600', '0.702', '0.628']\n",
      "['0.609', '0.621', '0.605', '0.612', '0.616', '0.748', '0.804', '0.770', '0.662', '0.632', '0.669', '0.658', '0.629', '0.612', '0.627', '0.646']\n",
      "['0.612', '0.725', '0.666', '0.642', '0.635', '0.636', '0.650', '0.626', '0.648', '0.635', '0.651', '0.631', '0.844', '0.616', '0.637', '0.824']\n",
      "['0.670', '0.784', '0.729', '0.740', '0.672', '0.667', '0.721', '0.810', '0.624', '0.646', '0.650', '0.663', '0.755', '0.649', '0.696', '0.754']\n",
      "['0.699', '0.886', '0.843', '0.700', '0.818', '0.814', '0.676', '0.882', '0.805', '0.791', '0.721', '0.703', '0.710', '0.755', '0.904', '0.900']\n",
      "['0.769', '0.882', '0.878', '0.875', '0.846', '0.818', '0.799', '0.875', '0.823', '0.809', '0.816', '0.842', '0.883', '0.840', '0.892', '0.879']\n",
      "['0.858', '0.850', '0.897', '0.890', '0.794', '0.813', '0.850', '0.848', '0.909', '0.861', '0.832', '0.856', '0.897', '0.874', '0.833', '0.864']\n",
      "['0.833', '0.827', '0.845', '0.882', '0.849', '0.867', '0.882', '0.855', '0.854', '0.859', '0.897', '0.882', '0.846', '0.851', '0.837', '0.916']\n",
      "['0.901', '0.877', '0.891', '0.891', '0.856', '0.854', '0.862', '0.881', '0.877', '0.817', '0.830', '0.828', '0.856', '0.878', '0.898', '0.877']\n",
      "['0.863', '0.860', '0.855', '0.899', '0.867', '0.890', '0.913', '0.901', '0.892', '0.883', '0.874', '0.858', '0.868', '0.866', '0.892', '0.881']\n",
      "['0.864', '0.805', '0.851', '0.847', '0.890', '0.882', '0.874', '0.895', '0.898', '0.913', '0.828', '0.896', '0.862', '0.825', '0.858', '0.857']\n",
      "['0.876', '0.884', '0.819', '0.836', '0.855', '0.845', '0.894', '0.872', '0.885', '0.899', '0.867', '0.867', '0.847', '0.861', '0.862', '0.866']\n",
      "['0.886', '0.888', '0.861', '0.869', '0.884', '0.877', '0.905', '0.890', '0.858', '0.852', '0.862', '0.865', '0.869', '0.859', '0.869', '0.865']\n",
      "['0.868', '0.869', '0.861', '0.871', '0.837', '0.864', '0.881', '0.849', '0.840', '0.856', '0.857', '0.862', '0.856', '0.856', '0.884', '0.853']\n",
      "['0.824', '0.856', '0.863', '0.859', '0.878', '0.873', '0.810', '0.875', '0.821', '0.857', '0.867', '0.881', '0.858', '0.870', '0.809', '0.766']\n",
      "['0.854', '0.854', '0.818', '0.874', '0.824', '0.787', '0.875', '0.867', '0.845', '0.855', '0.825', '0.851', '0.836', '0.824', '0.832', '0.831']\n",
      "['0.872', '0.861', '0.889', '0.890', '0.848', '0.850', '0.840', '0.832', '0.831', '0.862', '0.878', '0.865', '0.859', '0.847', '0.848', '0.839']\n",
      "['0.837', '0.854', '0.835', '0.841', '0.837', '0.851', '0.845', '0.843', '0.862', '0.854', '0.841', '0.853', '0.825', '0.818', '0.775', '0.776']\n",
      "['0.828', '0.814', '0.854', '0.858', '0.852', '0.843', '0.828', '0.818', '0.865', '0.872', '0.824', '0.832', '0.860', '0.866', '0.832', '0.812']\n",
      "['0.821', '0.797', '0.833', '0.836', '0.838', '0.849', '0.792', '0.831', '0.827', '0.833', '0.833', '0.832', '0.768', '0.827', '0.787', '0.787']\n",
      "['0.829', '0.792', '0.832', '0.837', '0.821', '0.843', '0.832', '0.791', '0.833', '0.841', '0.818', '0.839', '0.839', '0.801', '0.806', '0.841']\n",
      "['0.848', '0.818', '0.831', '0.827', '0.810', '0.812', '0.788', '0.783', '0.866', '0.847', '0.837', '0.834', '0.810', '0.852', '0.846', '0.804']\n",
      "['0.758', '0.802', '0.840', '0.813', '0.852', '0.820', '0.844', '0.813', '0.818', '0.822', '0.792', '0.839', '0.820', '0.825', '0.835', '0.837']\n",
      "['0.843', '0.848', '0.824', '0.829', '0.839', '0.846', '0.812', '0.826', '0.778', '0.813', '0.810', '0.828', '0.764', '0.713', '0.881', '0.866']\n",
      "['0.828', '0.827', '0.814', '0.812', '0.802', '0.820', '0.814', '0.805', '0.845', '0.843', '0.854', '0.860', '0.832', '0.800', '0.797', '0.853']\n",
      "['0.796', '0.833', '0.825', '0.835', '0.816', '0.839', '0.860', '0.859', '0.829', '0.835', '0.820', '0.804', '0.840', '0.828', '0.831', '0.818']\n",
      "['0.859', '0.865', '0.828', '0.835', '0.836', '0.859', '0.853', '0.855', '0.842', '0.845', '0.826', '0.814', '0.859', '0.843', '0.817', '0.767']\n",
      "\n",
      "Score:  0.9109707064905226\n",
      "\n",
      "===============  gemma_instruct  ===============\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "['0.513', '0.508', '0.510', '0.508', '0.559', '0.570', '0.617', '0.631', '0.674', '0.702', '0.698', '0.717', '0.706', '0.751', '0.794', '0.828', '0.862', '0.887', '0.904', '0.902', '0.910', '0.912', '0.915', '0.916', '0.925', '0.923', '0.921', '0.916', '0.916', '0.922', '0.917', '0.921', '0.915', '0.916', '0.921', '0.919', '0.924', '0.918', '0.921', '0.908', '0.912', '0.909']\n",
      "\n",
      "Score:  0.9231476163124641\n",
      "\n",
      "HEADS\n",
      "\n",
      "['0.531', '0.493', '0.466', '0.532', '0.504', '0.512', '0.531', '0.522', '0.509', '0.500', '0.517', '0.514', '0.524', '0.484', '0.498', '0.509']\n",
      "['0.513', '0.501', '0.497', '0.487', '0.508', '0.502', '0.502', '0.539', '0.485', '0.495', '0.511', '0.511', '0.497', '0.508', '0.538', '0.509']\n",
      "['0.499', '0.513', '0.520', '0.511', '0.509', '0.507', '0.504', '0.518', '0.527', '0.519', '0.525', '0.511', '0.513', '0.504', '0.513', '0.522']\n",
      "['0.499', '0.503', '0.523', '0.496', '0.516', '0.517', '0.524', '0.527', '0.499', '0.503', '0.512', '0.515', '0.508', '0.504', '0.512', '0.509']\n",
      "['0.519', '0.521', '0.520', '0.515', '0.546', '0.529', '0.547', '0.540', '0.516', '0.523', '0.538', '0.539', '0.516', '0.526', '0.527', '0.532']\n",
      "['0.545', '0.543', '0.546', '0.544', '0.550', '0.551', '0.566', '0.538', '0.538', '0.557', '0.540', '0.541', '0.545', '0.555', '0.561', '0.560']\n",
      "['0.548', '0.559', '0.585', '0.588', '0.567', '0.554', '0.608', '0.570', '0.568', '0.566', '0.581', '0.597', '0.566', '0.545', '0.554', '0.544']\n",
      "['0.590', '0.588', '0.592', '0.588', '0.577', '0.576', '0.595', '0.594', '0.569', '0.583', '0.573', '0.553', '0.578', '0.588', '0.585', '0.583']\n",
      "['0.597', '0.598', '0.593', '0.585', '0.604', '0.595', '0.604', '0.597', '0.576', '0.576', '0.618', '0.619', '0.582', '0.599', '0.632', '0.623']\n",
      "['0.599', '0.600', '0.621', '0.603', '0.599', '0.597', '0.601', '0.640', '0.614', '0.612', '0.597', '0.627', '0.628', '0.631', '0.608', '0.617']\n",
      "['0.602', '0.602', '0.663', '0.651', '0.591', '0.611', '0.684', '0.608', '0.585', '0.588', '0.652', '0.615', '0.616', '0.607', '0.627', '0.610']\n",
      "['0.611', '0.613', '0.604', '0.610', '0.606', '0.601', '0.630', '0.631', '0.657', '0.657', '0.644', '0.639', '0.607', '0.660', '0.651', '0.671']\n",
      "['0.678', '0.624', '0.660', '0.651', '0.635', '0.624', '0.631', '0.620', '0.612', '0.623', '0.643', '0.617', '0.633', '0.612', '0.683', '0.615']\n",
      "['0.684', '0.639', '0.623', '0.610', '0.650', '0.617', '0.622', '0.560', '0.621', '0.620', '0.710', '0.646', '0.685', '0.697', '0.658', '0.638']\n",
      "['0.759', '0.736', '0.628', '0.655', '0.732', '0.636', '0.680', '0.657', '0.622', '0.635', '0.611', '0.597', '0.761', '0.702', '0.763', '0.766']\n",
      "['0.642', '0.669', '0.746', '0.713', '0.817', '0.818', '0.644', '0.791', '0.696', '0.762', '0.713', '0.726', '0.635', '0.661', '0.761', '0.663']\n",
      "['0.665', '0.685', '0.653', '0.658', '0.692', '0.821', '0.794', '0.826', '0.800', '0.789', '0.774', '0.841', '0.752', '0.724', '0.748', '0.752']\n",
      "['0.728', '0.854', '0.874', '0.828', '0.768', '0.734', '0.808', '0.709', '0.837', '0.770', '0.867', '0.886', '0.883', '0.697', '0.748', '0.866']\n",
      "['0.896', '0.877', '0.907', '0.895', '0.889', '0.904', '0.887', '0.890', '0.859', '0.887', '0.890', '0.902', '0.908', '0.854', '0.895', '0.869']\n",
      "['0.878', '0.909', '0.892', '0.883', '0.901', '0.865', '0.878', '0.893', '0.892', '0.894', '0.889', '0.891', '0.879', '0.892', '0.913', '0.909']\n",
      "['0.870', '0.913', '0.906', '0.900', '0.870', '0.904', '0.896', '0.894', '0.902', '0.898', '0.888', '0.886', '0.901', '0.894', '0.915', '0.897']\n",
      "['0.899', '0.894', '0.911', '0.909', '0.905', '0.895', '0.903', '0.894', '0.911', '0.910', '0.894', '0.890', '0.910', '0.903', '0.886', '0.906']\n",
      "['0.894', '0.897', '0.889', '0.894', '0.887', '0.899', '0.905', '0.894', '0.884', '0.910', '0.914', '0.896', '0.900', '0.899', '0.898', '0.919']\n",
      "['0.912', '0.902', '0.916', '0.905', '0.904', '0.904', '0.907', '0.913', '0.910', '0.896', '0.905', '0.895', '0.887', '0.890', '0.910', '0.896']\n",
      "['0.902', '0.920', '0.890', '0.923', '0.903', '0.905', '0.926', '0.916', '0.906', '0.901', '0.901', '0.878', '0.908', '0.902', '0.922', '0.911']\n",
      "['0.912', '0.917', '0.911', '0.911', '0.916', '0.912', '0.914', '0.923', '0.931', '0.918', '0.918', '0.918', '0.890', '0.904', '0.897', '0.898']\n",
      "['0.914', '0.915', '0.923', '0.917', '0.921', '0.917', '0.907', '0.911', '0.909', '0.922', '0.912', '0.922', '0.917', '0.914', '0.908', '0.908']\n",
      "['0.922', '0.906', '0.909', '0.912', '0.911', '0.918', '0.911', '0.914', '0.901', '0.905', '0.908', '0.916', '0.912', '0.899', '0.913', '0.917']\n",
      "['0.912', '0.914', '0.907', '0.918', '0.911', '0.916', '0.911', '0.916', '0.895', '0.909', '0.918', '0.914', '0.910', '0.907', '0.920', '0.924']\n",
      "['0.909', '0.892', '0.880', '0.906', '0.910', '0.904', '0.917', '0.897', '0.887', '0.901', '0.886', '0.914', '0.906', '0.888', '0.886', '0.833']\n",
      "['0.910', '0.911', '0.878', '0.904', '0.814', '0.893', '0.916', '0.912', '0.910', '0.896', '0.902', '0.895', '0.895', '0.862', '0.906', '0.892']\n",
      "['0.893', '0.907', '0.914', '0.918', '0.908', '0.913', '0.899', '0.895', '0.891', '0.895', '0.917', '0.910', '0.899', '0.874', '0.912', '0.907']\n",
      "['0.913', '0.899', '0.904', '0.906', '0.896', '0.901', '0.904', '0.905', '0.898', '0.886', '0.895', '0.909', '0.874', '0.828', '0.821', '0.809']\n",
      "['0.887', '0.879', '0.906', '0.897', '0.906', '0.907', '0.900', '0.855', '0.908', '0.912', '0.895', '0.892', '0.902', '0.916', '0.812', '0.832']\n",
      "['0.860', '0.858', '0.899', '0.905', '0.882', '0.908', '0.887', '0.899', '0.898', '0.849', '0.906', '0.907', '0.829', '0.902', '0.897', '0.889']\n",
      "['0.895', '0.875', '0.900', '0.905', '0.904', '0.897', '0.903', '0.882', '0.894', '0.893', '0.872', '0.912', '0.897', '0.906', '0.817', '0.894']\n",
      "['0.883', '0.894', '0.900', '0.902', '0.906', '0.839', '0.845', '0.889', '0.889', '0.904', '0.895', '0.904', '0.890', '0.906', '0.899', '0.899']\n",
      "['0.801', '0.863', '0.897', '0.899', '0.924', '0.916', '0.902', '0.895', '0.900', '0.902', '0.889', '0.902', '0.866', '0.879', '0.871', '0.890']\n",
      "['0.908', '0.902', '0.898', '0.898', '0.884', '0.879', '0.895', '0.858', '0.898', '0.904', '0.889', '0.892', '0.886', '0.852', '0.912', '0.914']\n",
      "['0.898', '0.902', '0.892', '0.905', '0.898', '0.897', '0.895', '0.886', '0.904', '0.908', '0.906', '0.909', '0.905', '0.882', '0.909', '0.890']\n",
      "['0.901', '0.889', '0.893', '0.897', '0.905', '0.892', '0.887', '0.899', '0.893', '0.896', '0.904', '0.905', '0.901', '0.906', '0.883', '0.905']\n",
      "['0.911', '0.916', '0.891', '0.895', '0.901', '0.905', '0.897', '0.894', '0.904', '0.898', '0.893', '0.881', '0.901', '0.900', '0.912', '0.908']\n",
      "\n",
      "Score:  0.9254451464675473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def score_accuracy(data, top_k=None):\n",
    "    \n",
    "    data = t.tensor(data) if not isinstance(data, t.Tensor) else data\n",
    "    if top_k is not None:\n",
    "        # Flatten to 1D and select top-k\n",
    "        top_values = t.topk(data.flatten(), top_k).values\n",
    "        score = top_values.mean().item()\n",
    "    else:\n",
    "        score = data.mean().item()\n",
    "    return score\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "for model in models:\n",
    "    path = (PATH / model / 'ACCURACY')\n",
    "\n",
    "    print(\"=============== \", model, \" ===============\")\n",
    "    print() \n",
    "    print(\"RESIDUAL\")\n",
    "    print()\n",
    "\n",
    "    accuracies = t.load(path / 'accuracies_residual', weights_only=False)\n",
    "    print([f\"{x:.3f}\" for x in accuracies])\n",
    "    print()\n",
    "\n",
    "    print(\"Score: \", score_accuracy(accuracies, top_k=top_k))\n",
    "    print()\n",
    "    \n",
    "    print(\"HEADS\")\n",
    "    print()\n",
    "    accuracies = t.load(path / 'accuracies_heads', weights_only=False)\n",
    "    for row in accuracies:\n",
    "        print([f\"{x:.3f}\" for x in row])\n",
    "    print()\n",
    "    print(\"Score: \", score_accuracy(accuracies, top_k=top_k))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17848ad",
   "metadata": {},
   "source": [
    "## Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed583efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma\n",
      "\n",
      "RESIDUAL\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 31.0], 'k': [7], 'alpha_control': [0, 35.0], 'k_control': [7], 'clean': (array([[0.0464907 , 0.79244151]]), array([[0.57519531, 0.77001953]])), 'control': (array([[0.0464907 , 0.20875825]]), array([[0.57519531, 0.62011719]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -33.0], 'k': [22], 'alpha_control': [0, -33.0], 'k_control': [22], 'clean': (array([[0.        , 0.19607241]]), array([[0.33813477, 0.76855469]])), 'control': (array([[0., 0.]]), array([[0.33813477, 0.390625  ]]))})\n",
      "\n",
      "HEADS\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0]\n",
      "[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 65, 70, 75]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 12.0], 'k': [15], 'alpha_control': [0, 30.0], 'k_control': [70], 'clean': (array([[0.04676039, 0.99969438]]), array([[0.57470703, 0.86181641]])), 'control': (array([[0.04676039, 0.00030562]]), array([[0.57470703, 0.62207031]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -10.0], 'k': [60], 'alpha_control': [0, -30.0], 'k_control': [70], 'clean': (array([[0.        , 0.99939976]]), array([[0.33666992, 0.80566406]])), 'control': (array([[0., 0.]]), array([[0.33666992, 0.41674805]]))})\n",
      "False\n",
      "\n",
      "gemma_instruct\n",
      "\n",
      "RESIDUAL\n",
      "dict_keys(['fixed'])\n",
      "dict_keys([])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {})\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 30.0], 'k': [16], 'alpha_control': [0, 30.0], 'k_control': [16], 'clean': (array([[0.06232814, 0.47968225]]), array([[0.15722656, 0.81542969]])), 'control': (array([[0.06232814, 0.06813321]]), array([[0.15722656, 0.10839844]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -30.0], 'k': [26], 'alpha_control': [0, -30.0], 'k_control': [30], 'clean': (array([[0.01149425, 0.97882638]]), array([[0.03057861, 0.90380859]])), 'control': (array([[0.01149425, 0.00725953]]), array([[0.03057861, 0.03952026]]))})\n",
      "\n",
      "HEADS\n",
      "dict_keys(['fixed'])\n",
      "dict_keys([])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {})\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 26.0], 'k': [5], 'alpha_control': [0, 30.0], 'k_control': [40], 'clean': (array([[0.06232814, 0.98564009]]), array([[0.15722656, 0.97802734]])), 'control': (array([[0.06232814, 0.57409105]]), array([[0.15722656, 0.82861328]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -14.0], 'k': [25], 'alpha_control': [0, -30.0], 'k_control': [70], 'clean': (array([[0.01149425, 0.99727768]]), array([[0.03057861, 0.97802734]])), 'control': (array([[0.01149425, 0.00120992]]), array([[0.03057861, 0.27905273]]))})\n",
      "False\n",
      "\n",
      "llama\n",
      "\n",
      "RESIDUAL\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 9.0], 'k': [1], 'clean': (array([[0.01100244, 0.9452934 ]]), array([[0.33496094, 0.71728516]])), 'control': (array([[0.01100244, 0.27444988]]), array([[0.33496094, 0.48388672]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -6.0], 'k': [1], 'clean': (array([[0.37364946, 0.742497  ]]), array([[0.49169922, 0.55957031]])), 'control': (array([[0.37364946, 0.30522209]]), array([[0.49169922, 0.47631836]]))})\n",
      "\n",
      "HEADS\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 2.0], 'k': [40], 'alpha_control': [0, 2.0], 'k_control': [40], 'clean': (array([[0.28728606, 0.96974328]]), array([[0.45385742, 0.82958984]])), 'control': (array([[0.28728606, 0.2096577 ]]), array([[0.45385742, 0.39770508]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -9.0], 'k': [1], 'alpha_control': [0, -3.0], 'k_control': [20], 'clean': (array([[0.09753902, 0.28361345]]), array([[0.39331055, 0.48168945]])), 'control': (array([[0.09753902, 0.88355342]]), array([[0.39331055, 0.61621094]]))})\n",
      "False\n",
      "\n",
      "llama_instruct\n",
      "\n",
      "RESIDUAL\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 3.0], 'k': [3], 'clean': (array([[0.05042787, 0.98685819]]), array([[0.10522461, 0.97753906]])), 'control': (array([[0.05042787, 0.049511  ]]), array([[0.10522461, 0.17724609]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -5.0], 'k': [2], 'clean': (array([[0.19837935, 0.95078031]]), array([[0.12756348, 0.92675781]])), 'control': (array([[0.19837935, 0.23319328]]), array([[0.12756348, 0.17211914]]))})\n",
      "\n",
      "HEADS\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 2.0], 'k': [10], 'clean': (array([[0.05042787, 0.99724939]]), array([[0.10522461, 0.98388672]])), 'control': (array([[0.05042787, 0.81448655]]), array([[0.10522461, 0.81542969]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -3.0], 'k': [10], 'clean': (array([[0.19837935, 1.        ]]), array([[0.12756348, 0.98046875]])), 'control': (array([[0.19837935, 0.9819928 ]]), array([[0.12756348, 0.91259766]]))})\n",
      "False\n",
      "\n",
      "gpt-j\n",
      "\n",
      "RESIDUAL\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 22.0, 24.0, 26.0, 28.0, 30.0]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 4.0], 'k': [15], 'alpha_control': [0, 16.0], 'k_control': [12], 'clean': (array([[0.01314181, 0.00672372]]), array([[0.72265625, 0.73681641]])), 'control': (array([[0.01314181, 0.12775061]]), array([[0.72265625, 0.75097656]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -30.0], 'k': [15], 'alpha_control': [0, -30.0], 'k_control': [15], 'clean': (array([[0., 0.]]), array([[0.27075195, 0.50195312]])), 'control': (array([[0., 0.]]), array([[0.27075195, 0.45141602]]))})\n",
      "\n",
      "HEADS\n",
      "dict_keys(['sweep', 'fixed'])\n",
      "dict_keys(['alphas', 'ks', 'ft', 'tf'])\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
      "dict_keys(['ft', 'tf'])\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, 2.0], 'k': [5], 'alpha_control': [0, 9.0], 'k_control': [20], 'clean': (array([[0.01314181, 0.01191932]]), array([[0.72265625, 0.72802734]])), 'control': (array([[0.01314181, 0.7396088 ]]), array([[0.72265625, 0.86621094]]))})\n",
      "defaultdict(<function nested_dict at 0x000001834ACBE200>, {'alpha': [0, -10.0], 'k': [50], 'alpha_control': [0, -10.0], 'k_control': [50], 'clean': (array([[0., 0.]]), array([[0.27075195, 0.42089844]])), 'control': (array([[0., 0.]]), array([[0.27075195, 0.48364258]]))})\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['gemma', 'gemma_instruct', 'llama', 'llama_instruct', 'gpt-j']\n",
    "\n",
    "d_coeff = { 'gemma': 256/3584,                          # D_head/D_model\n",
    "           'gemma_instruct': 256/3584,\n",
    "           'llama': 128/4096,\n",
    "           'llama_instruct': 128/4096,\n",
    "           'gpt-j': 256/4096 \n",
    "           }\n",
    "\n",
    "def recover_pd(y, scale_proba=5.0, eps=1e-6):\n",
    "\n",
    "    # WARNING: This function is necessary since in our experiment's run we logged scaled/squished results during the data collection\n",
    "    # If you want to recover your own data and then check the results, make sure to change the return type of the function\n",
    "\n",
    "    \"\"\"\n",
    "    Approximate recovery of tot_pd from sigmoid output y.\n",
    "    Assumes y in (0, 1).\n",
    "    \"\"\"\n",
    "    recovered_y = y\n",
    "    recovered_y = t.tensor(recovered_y, dtype=t.float32)\n",
    "    recovered_y = t.clamp(recovered_y, eps, 1 - eps)\n",
    "    return (t.log(recovered_y / (1 - recovered_y)) / scale_proba).item()\n",
    "    # return y\n",
    "\n",
    "vectorized_stre = np.vectorize(recover_pd)\n",
    "\n",
    "model_stats_residual = {'gemma': {'ft_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}},'tf_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}}},\n",
    "    'gemma_instruct': {'ft_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}},'tf_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}}},\n",
    "    'llama' : {'ft_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}},'tf_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}}},\n",
    "    'llama_instruct': {'ft_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}},'tf_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}}},\n",
    "    'gpt-j': {'ft_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}},'tf_og_probas': {'clean': {'result':(), 'a':0, 'k':0},'control': {'result':(), 'a':0, 'k':0}}}\n",
    "}\n",
    "\n",
    "model_stats_heads = copy.deepcopy(model_stats_residual)\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    print(model)\n",
    "    print()\n",
    "    print(\"RESIDUAL\")\n",
    "    checkfile = t.load(PATH / model / 'USE' / 'intervention_scores_residual', weights_only=False)\n",
    "    print(checkfile.keys())\n",
    "    print(checkfile['sweep'].keys())\n",
    "    print(checkfile['sweep']['alphas'])\n",
    "    print(checkfile['sweep']['ks'])\n",
    "    print(checkfile['fixed'].keys())\n",
    "    print(checkfile['fixed']['ft'])\n",
    "    print(checkfile['fixed']['tf'])\n",
    "    model_stats_residual[model]['ft_og_probas']['clean']['result'] = (recover_pd(checkfile['fixed']['ft']['clean'][1][0][0]), recover_pd(checkfile['fixed']['ft']['clean'][1][0][1]))\n",
    "    model_stats_residual[model]['ft_og_probas']['clean']['a'] = checkfile['fixed']['ft']['alpha'][1]\n",
    "    model_stats_residual[model]['ft_og_probas']['clean']['k'] = checkfile['fixed']['ft']['k'][0]\n",
    "    model_stats_residual[model]['ft_og_probas']['control']['result'] = (recover_pd(checkfile['fixed']['ft']['control'][1][0][0]), recover_pd(checkfile['fixed']['ft']['control'][1][0][1]))\n",
    "    model_stats_residual[model]['ft_og_probas']['control']['a'] = checkfile['fixed']['ft']['alpha_control'][1]\n",
    "    model_stats_residual[model]['ft_og_probas']['control']['k'] = checkfile['fixed']['ft']['k_control'][0]\n",
    "    model_stats_residual[model]['tf_og_probas']['clean']['result'] = (recover_pd(checkfile['fixed']['tf']['clean'][1][0][0]), recover_pd(checkfile['fixed']['tf']['clean'][1][0][1]))\n",
    "    model_stats_residual[model]['tf_og_probas']['clean']['a'] = checkfile['fixed']['tf']['alpha'][1]\n",
    "    model_stats_residual[model]['tf_og_probas']['clean']['k'] = checkfile['fixed']['tf']['k'][0]\n",
    "    model_stats_residual[model]['tf_og_probas']['control']['result'] = (recover_pd(checkfile['fixed']['tf']['control'][1][0][0]), recover_pd(checkfile['fixed']['tf']['control'][1][0][1]))\n",
    "    model_stats_residual[model]['tf_og_probas']['control']['a'] = checkfile['fixed']['tf']['alpha_control'][1]\n",
    "    model_stats_residual[model]['tf_og_probas']['control']['k'] = checkfile['fixed']['ft']['k_control'][0]\n",
    "\n",
    "    print()\n",
    "    print(\"HEADS\")\n",
    "    checkfile = t.load(PATH / model / 'USE' / 'intervention_scores_heads', weights_only=False)\n",
    "    print(checkfile.keys())\n",
    "    print(checkfile['sweep'].keys())\n",
    "    print(checkfile['sweep']['alphas'])\n",
    "    print(checkfile['sweep']['ks'])\n",
    "    print(checkfile['fixed'].keys())\n",
    "    print(checkfile['fixed']['ft'])\n",
    "    print(checkfile['fixed']['tf'])\n",
    "    model_stats_heads[model]['ft_og_probas']['clean']['result'] = (recover_pd(checkfile['fixed']['ft']['clean'][1][0][0]), recover_pd(checkfile['fixed']['ft']['clean'][1][0][1]))\n",
    "    model_stats_heads[model]['ft_og_probas']['clean']['a'] = checkfile['fixed']['ft']['alpha'][1]\n",
    "    model_stats_heads[model]['ft_og_probas']['clean']['k'] = checkfile['fixed']['ft']['k'][0]\n",
    "    model_stats_heads[model]['ft_og_probas']['control']['result'] = (recover_pd(checkfile['fixed']['ft']['control'][1][0][0]), recover_pd(checkfile['fixed']['ft']['control'][1][0][1]))\n",
    "    model_stats_heads[model]['ft_og_probas']['control']['a'] = checkfile['fixed']['ft']['alpha_control'][1]\n",
    "    model_stats_heads[model]['ft_og_probas']['control']['k'] = checkfile['fixed']['ft']['k_control'][0]\n",
    "    model_stats_heads[model]['tf_og_probas']['clean']['result'] = (recover_pd(checkfile['fixed']['tf']['clean'][1][0][0]), recover_pd(checkfile['fixed']['tf']['clean'][1][0][1]))\n",
    "    model_stats_heads[model]['tf_og_probas']['clean']['a'] = checkfile['fixed']['tf']['alpha'][1]\n",
    "    model_stats_heads[model]['tf_og_probas']['clean']['k'] = checkfile['fixed']['tf']['k'][0]\n",
    "    model_stats_heads[model]['tf_og_probas']['control']['result'] = (recover_pd(checkfile['fixed']['tf']['control'][1][0][0]), recover_pd(checkfile['fixed']['tf']['control'][1][0][1]))\n",
    "    model_stats_heads[model]['tf_og_probas']['control']['a'] = checkfile['fixed']['tf']['alpha_control'][1]\n",
    "    model_stats_heads[model]['tf_og_probas']['control']['k'] = checkfile['fixed']['ft']['k_control'][0]\n",
    "    print(model_stats_residual[model]['ft_og_probas']['clean'] == model_stats_heads[model]['ft_og_probas']['clean'])\n",
    "    print()\n",
    "\n",
    "\n",
    "model_stats_residual['llama']['ft_og_probas']['control']['a'] = 9\n",
    "model_stats_residual['llama']['ft_og_probas']['control']['k'] = 8\n",
    "model_stats_residual['llama']['tf_og_probas']['control']['a'] = 1\n",
    "model_stats_residual['llama']['tf_og_probas']['control']['k'] = 2\n",
    "model_stats_residual['llama_instruct']['ft_og_probas']['control']['a'] = 3\n",
    "model_stats_residual['llama_instruct']['ft_og_probas']['control']['k'] = 3\n",
    "model_stats_residual['llama_instruct']['tf_og_probas']['control']['a'] = 2\n",
    "model_stats_residual['llama_instruct']['tf_og_probas']['control']['k'] = 6\n",
    "model_stats_heads['llama_instruct']['ft_og_probas']['control']['a'] = 6\n",
    "model_stats_heads['llama_instruct']['ft_og_probas']['control']['k'] = 25\n",
    "model_stats_heads['llama_instruct']['tf_og_probas']['control']['a'] = 5\n",
    "model_stats_heads['llama_instruct']['tf_og_probas']['control']['k'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f623a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESIDUAL\n",
      "gemma\n",
      "F->T\n",
      "E 0.181 \n",
      "E-> 5.145\n",
      "Alpha, K, Alpha_Control, K_Control 31.0 7 35.0 7\n",
      "S 14.730919862656235\n",
      "T->F\n",
      "E 0.374\n",
      "E-> 4.653\n",
      "Alpha, K, Alpha_Control, K_Control -33.0 22 -33.0 7\n",
      "S 26.94438717061496\n",
      "Avg\n",
      "E 0.278\n",
      "E-> 4.899\n",
      "\n",
      "Score\n",
      "E 0.667\n",
      "E-> 0.642\n",
      "gemma_instruct\n",
      "F->T\n",
      "E 0.633 \n",
      "E-> 7.391\n",
      "Alpha, K, Alpha_Control, K_Control 30.0 16 30.0 16\n",
      "S 21.908902300206645\n",
      "T->F\n",
      "E 1.139\n",
      "E-> 16.814\n",
      "Alpha, K, Alpha_Control, K_Control -30.0 26 -30.0 16\n",
      "S 27.92848008753788\n",
      "Avg\n",
      "E 0.886\n",
      "E-> 12.102\n",
      "\n",
      "Score\n",
      "E 0.902\n",
      "E-> 0.841\n",
      "llama\n",
      "F->T\n",
      "E 0.323 \n",
      "E-> 7.360\n",
      "Alpha, K, Alpha_Control, K_Control 9.0 1 9 8\n",
      "S 3.0\n",
      "T->F\n",
      "E 0.055\n",
      "E-> 2.556\n",
      "Alpha, K, Alpha_Control, K_Control -6.0 1 1 2\n",
      "S 2.449489742783178\n",
      "Avg\n",
      "E 0.189\n",
      "E-> 4.958\n",
      "\n",
      "Score\n",
      "E 0.616\n",
      "E-> 0.644\n",
      "llama_instruct\n",
      "F->T\n",
      "E 1.183 \n",
      "E-> 9.769\n",
      "Alpha, K, Alpha_Control, K_Control 3.0 3 3 3\n",
      "S 3.0\n",
      "T->F\n",
      "E 0.892\n",
      "E-> 13.882\n",
      "Alpha, K, Alpha_Control, K_Control -5.0 2 2 6\n",
      "S 3.1622776601683795\n",
      "Avg\n",
      "E 1.037\n",
      "E-> 11.825\n",
      "\n",
      "Score\n",
      "E 0.930\n",
      "E-> 0.835\n",
      "gpt-j\n",
      "F->T\n",
      "E 0.014 \n",
      "E-> 0.879\n",
      "Alpha, K, Alpha_Control, K_Control 4.0 15 16.0 12\n",
      "S 7.745966692414834\n",
      "T->F\n",
      "E 0.200\n",
      "E-> 1.122\n",
      "Alpha, K, Alpha_Control, K_Control -30.0 15 -30.0 12\n",
      "S 21.213203435596427\n",
      "Avg\n",
      "E 0.107\n",
      "E-> 1.001\n",
      "\n",
      "Score\n",
      "E 0.567\n",
      "E-> 0.500\n",
      "HEADS\n",
      "gemma\n",
      "F->T\n",
      "E 0.306\n",
      "E-> 26.482\n",
      "Alpha, K, Alpha_Control, K_Control 12.0 15 30.0 70\n",
      "S 3.585685828003181\n",
      "T->F\n",
      "E 0.420\n",
      "E-> 11.488\n",
      "Alpha, K, Alpha_Control, K_Control -10.0 60 -30.0 70\n",
      "S 6.546536707079771\n",
      "Avg\n",
      "\n",
      "Score\n",
      "E 0.712\n",
      "E-> 0.937\n",
      "gemma_instruct\n",
      "F->T\n",
      "E 1.095\n",
      "E-> 5.110\n",
      "Alpha, K, Alpha_Control, K_Control 26.0 5 30.0 40\n",
      "S 3.04724700110022\n",
      "T->F\n",
      "E 1.450\n",
      "E-> 5.356\n",
      "Alpha, K, Alpha_Control, K_Control -14.0 25 -30.0 40\n",
      "S 5.0\n",
      "Avg\n",
      "\n",
      "Score\n",
      "E 0.960\n",
      "E-> 0.654\n",
      "llama\n",
      "F->T\n",
      "E 0.354\n",
      "E-> 7.688\n",
      "Alpha, K, Alpha_Control, K_Control 2.0 40 2.0 40\n",
      "S 1.5811388300841898\n",
      "T->F\n",
      "E 0.072\n",
      "E-> 1.450\n",
      "Alpha, K, Alpha_Control, K_Control -9.0 1 -3.0 40\n",
      "S 0.5303300858899106\n",
      "Avg\n",
      "\n",
      "Score\n",
      "E 0.630\n",
      "E-> 0.631\n",
      "llama_instruct\n",
      "F->T\n",
      "E 1.250\n",
      "E-> 4.722\n",
      "Alpha, K, Alpha_Control, K_Control 2.0 10 6 25\n",
      "S 0.7905694150420949\n",
      "T->F\n",
      "E 1.168\n",
      "E-> 2.163\n",
      "Alpha, K, Alpha_Control, K_Control -3.0 10 5 15\n",
      "S 0.9682458365518543\n",
      "Avg\n",
      "\n",
      "Score\n",
      "E 0.954\n",
      "E-> 0.591\n",
      "gpt-j\n",
      "F->T\n",
      "E 0.005\n",
      "E-> 0.126\n",
      "Alpha, K, Alpha_Control, K_Control 2.0 5 9.0 20\n",
      "S 0.7905694150420949\n",
      "T->F\n",
      "E 0.134\n",
      "E-> 0.459\n",
      "Alpha, K, Alpha_Control, K_Control -10.0 50 -10.0 20\n",
      "S 5.5901699437494745\n",
      "Avg\n",
      "\n",
      "Score\n",
      "E 0.544\n",
      "E-> 0.473\n"
     ]
    }
   ],
   "source": [
    "def stre(a, k, model_name, heads=False):\n",
    "    k = k*d_coeff[model_name] if heads else k\n",
    "    return np.abs(k*a)\n",
    "\n",
    "def score(num, k=2.5):\n",
    "    return 1/(1+np.exp(-k*num))\n",
    "\n",
    "def score_e_(num, k=0.15):\n",
    "    return 1/(1+np.exp(-k*(num - 1)))\n",
    "\n",
    "for model, model_data in model_stats_residual.items():\n",
    "    for block in model_data.values():\n",
    "        for entry in block.values():\n",
    "            entry[\"s\"] = stre(entry[\"a\"], entry[\"k\"], model)\n",
    "            entry[\"e\"] = entry[\"result\"][1] - entry[\"result\"][0]\n",
    "        block[\"effect\"] = block[\"clean\"][\"e\"]\n",
    "        block[\"effect_\"] = np.abs(( block[\"clean\"][\"e\"] / np.sqrt(block[\"clean\"][\"s\"]) ) / (block[\"control\"][\"e\"] / np.sqrt(block[\"control\"][\"s\"]) ))\n",
    "        \n",
    "for model, model_data in model_stats_heads.items():\n",
    "    for block in model_data.values():\n",
    "        for entry in block.values():\n",
    "            entry[\"s\"] = stre(entry[\"a\"], entry[\"k\"], model, heads=True)\n",
    "            entry[\"e\"] = entry[\"result\"][1] - entry[\"result\"][0]\n",
    "        block[\"effect\"] = block[\"clean\"][\"e\"]\n",
    "        block[\"effect_\"] = np.abs(( block[\"clean\"][\"e\"] / np.sqrt(block[\"clean\"][\"s\"]) ) / (block[\"control\"][\"e\"] / np.sqrt(block[\"control\"][\"s\"]) ))\n",
    "\n",
    "print(\"RESIDUAL\")\n",
    "for model, values in model_stats_residual.items():\n",
    "    print(model)\n",
    "    print('F->T')\n",
    "    print(\"E\", f\"{values['ft_og_probas']['effect']:.3f} \")\n",
    "    print(\"E->\", f\"{values['ft_og_probas']['effect_']:.3f}\")\n",
    "    print(\"Alpha, K, Alpha_Control, K_Control\", values['ft_og_probas']['clean']['a'], values['ft_og_probas']['clean']['k'], values['ft_og_probas']['control']['a'], values['ft_og_probas']['control']['k'])\n",
    "    print(\"S\", np.sqrt(values['ft_og_probas']['clean']['s']))\n",
    "    print('T->F')\n",
    "    print(\"E\", f\"{values['tf_og_probas']['effect']:.3f}\")\n",
    "    print(\"E->\", f\"{values['tf_og_probas']['effect_']:.3f}\")\n",
    "    print(\"Alpha, K, Alpha_Control, K_Control\", values['tf_og_probas']['clean']['a'], values['tf_og_probas']['clean']['k'], values['tf_og_probas']['control']['a'], values['tf_og_probas']['control']['k'])\n",
    "    print(\"S\", np.sqrt(values['tf_og_probas']['clean']['s']))\n",
    "\n",
    "    print('Avg')\n",
    "    print(\"E\", f\"{np.mean([values['ft_og_probas']['effect'], values['tf_og_probas']['effect']]):.3f}\")\n",
    "    print(\"E->\", f\"{np.mean([values['ft_og_probas']['effect_'], values['tf_og_probas']['effect_']]):.3f}\")\n",
    "    print()\n",
    "    print('Score')\n",
    "    print(\"E\", f\"{score(np.mean([values['ft_og_probas']['effect'], values['tf_og_probas']['effect']])):.3f}\")\n",
    "    print(\"E->\", f\"{score_e_(np.mean([values['ft_og_probas']['effect_'], values['tf_og_probas']['effect_']])):.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"HEADS\")\n",
    "for model, values in model_stats_heads.items():\n",
    "    print(model)\n",
    "    print('F->T')\n",
    "    print(\"E\", f\"{values['ft_og_probas']['effect']:.3f}\")\n",
    "    print(\"E->\", f\"{values['ft_og_probas']['effect_']:.3f}\")\n",
    "    print(\"Alpha, K, Alpha_Control, K_Control\", values['ft_og_probas']['clean']['a'], values['ft_og_probas']['clean']['k'], values['ft_og_probas']['control']['a'], values['ft_og_probas']['control']['k'])\n",
    "    print(\"S\", np.sqrt(values['ft_og_probas']['clean']['s']))\n",
    "    print('T->F')\n",
    "    print(\"E\", f\"{values['tf_og_probas']['effect']:.3f}\")\n",
    "    print(\"E->\", f\"{values['tf_og_probas']['effect_']:.3f}\")\n",
    "    print(\"Alpha, K, Alpha_Control, K_Control\", values['tf_og_probas']['clean']['a'], values['tf_og_probas']['clean']['k'], values['tf_og_probas']['control']['a'], values['tf_og_probas']['control']['k'])\n",
    "    print(\"S\", np.sqrt(values['tf_og_probas']['clean']['s']))\n",
    "    print('Avg')\n",
    "    print()\n",
    "    print('Score')\n",
    "    print(\"E\", f\"{score(np.mean([values['ft_og_probas']['effect'], values['tf_og_probas']['effect']])):.3f}\")\n",
    "    print(\"E->\", f\"{score_e_(np.mean([values['ft_og_probas']['effect_'], values['tf_og_probas']['effect_']])):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f21b4",
   "metadata": {},
   "source": [
    "## Coherence (Probabilistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESIDUAL\n",
      "llama \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.8406, dtype=torch.float64), tensor(0.7642, dtype=torch.float64))), ('or', (tensor(0.7754), 0.5)), ('and', (tensor(0.8359), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.6961), tensor(0.6841))), ('or', (tensor(0.8477), 0.5)), ('and', (tensor(0.9546), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.8284), tensor(0.7898))), ('or', (tensor(0.4741), 0.5)), ('and', (tensor(0.9272), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8965, dtype=torch.float64), tensor(0.8724, dtype=torch.float64))), ('or', (tensor(0.7261), 0.5)), ('and', (tensor(0.3882), 0.5))]))])\n",
      "llama_instruct \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.7324, dtype=torch.float64), tensor(0.7080, dtype=torch.float64))), ('or', (tensor(0.7925), 0.5)), ('and', (tensor(0.8516), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.7510), tensor(0.7137))), ('or', (tensor(0.8091), 0.5)), ('and', (tensor(0.8496), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.8112), tensor(0.7456))), ('or', (tensor(0.9062), 0.5)), ('and', (tensor(0.8149), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8985, dtype=torch.float64), tensor(0.8079, dtype=torch.float64))), ('or', (tensor(0.7393), 0.5)), ('and', (tensor(0.4741), 0.5))]))])\n",
      "gpt-j \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.7099, dtype=torch.float64), tensor(0.7535, dtype=torch.float64))), ('or', (tensor(0.7896), 0.5)), ('and', (tensor(0.8164), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.5192), tensor(0.5213))), ('or', (tensor(0.9766), 0.5)), ('and', (tensor(0.9990), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.5607), tensor(0.5619))), ('or', (tensor(0.9395), 0.5)), ('and', (tensor(0.9727), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8629, dtype=torch.float64), tensor(0.8679, dtype=torch.float64))), ('or', (tensor(0.0640), 0.5)), ('and', (tensor(0.8818), 0.5))]))])\n",
      "gemma \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.7919, dtype=torch.float64), tensor(0.7334, dtype=torch.float64))), ('or', (tensor(0.7925), 0.5)), ('and', (tensor(0.7988), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.5002), tensor(0.5002))), ('or', (tensor(0.9995), 0.5)), ('and', (tensor(0.9536), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.7198), tensor(0.6740))), ('or', (tensor(0.8638), 0.5)), ('and', (tensor(0.9136), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.9170, dtype=torch.float64), tensor(0.8919, dtype=torch.float64))), ('or', (tensor(0.5103), 0.5)), ('and', (tensor(0.2793), 0.5))]))])\n",
      "gemma_instruct \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.8279, dtype=torch.float64), tensor(0.7099, dtype=torch.float64))), ('or', (tensor(0.7554), 0.5)), ('and', (tensor(0.8130), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.5003), tensor(0.5003))), ('or', (tensor(0.9741), 0.5)), ('and', (tensor(0.9062), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.7895), tensor(0.6998))), ('or', (tensor(0.9258), 0.5)), ('and', (tensor(0.7427), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8513, dtype=torch.float64), tensor(0.7322, dtype=torch.float64))), ('or', (tensor(0.6714), 0.5)), ('and', (tensor(0.5132), 0.5))]))])\n",
      "HEADS\n",
      "llama \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.8626, dtype=torch.float64), tensor(0.8782, dtype=torch.float64))), ('or', (tensor(0.8413), 0.5)), ('and', (tensor(0.8115), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.6379), tensor(0.6948))), ('or', (tensor(0.9312), 0.5)), ('and', (tensor(0.9370), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.8284), tensor(0.7898))), ('or', (tensor(0.4741), 0.5)), ('and', (tensor(0.9272), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8965, dtype=torch.float64), tensor(0.8724, dtype=torch.float64))), ('or', (tensor(0.7261), 0.5)), ('and', (tensor(0.3882), 0.5))]))])\n",
      "llama_instruct \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.8743, dtype=torch.float64), tensor(0.8331, dtype=torch.float64))), ('or', (tensor(0.7661), 0.5)), ('and', (tensor(0.7983), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.7080), tensor(0.6811))), ('or', (tensor(0.9199), 0.5)), ('and', (tensor(0.9307), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.8112), tensor(0.7456))), ('or', (tensor(0.9062), 0.5)), ('and', (tensor(0.8149), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8985, dtype=torch.float64), tensor(0.8079, dtype=torch.float64))), ('or', (tensor(0.7393), 0.5)), ('and', (tensor(0.4741), 0.5))]))])\n",
      "gpt-j \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.9309, dtype=torch.float64), tensor(0.9569, dtype=torch.float64))), ('or', (tensor(0.9087), 0.5)), ('and', (tensor(0.9302), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.6687), tensor(0.7549))), ('or', (tensor(0.9775), 0.5)), ('and', (tensor(0.9653), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.5607), tensor(0.5619))), ('or', (tensor(0.9395), 0.5)), ('and', (tensor(0.9727), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8629, dtype=torch.float64), tensor(0.8679, dtype=torch.float64))), ('or', (tensor(0.0640), 0.5)), ('and', (tensor(0.8818), 0.5))]))])\n",
      "gemma \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.8727, dtype=torch.float64), tensor(0.8400, dtype=torch.float64))), ('or', (tensor(0.7920), 0.5)), ('and', (tensor(0.7485), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.5463), tensor(0.5464))), ('or', (tensor(0.9521), 0.5)), ('and', (tensor(0.9756), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.7198), tensor(0.6740))), ('or', (tensor(0.8638), 0.5)), ('and', (tensor(0.9136), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.9170, dtype=torch.float64), tensor(0.8919, dtype=torch.float64))), ('or', (tensor(0.5103), 0.5)), ('and', (tensor(0.2793), 0.5))]))])\n",
      "gemma_instruct \n",
      " OrderedDict([('logistic_regression', OrderedDict([('neg', (tensor(0.9072, dtype=torch.float64), tensor(0.7837, dtype=torch.float64))), ('or', (tensor(0.7319), 0.5)), ('and', (tensor(0.7539), 0.5))])), ('mmp', OrderedDict([('neg', (tensor(0.7185), tensor(0.6728))), ('or', (tensor(0.8281), 0.5)), ('and', (tensor(0.8569), 0.5))])), ('self_report', OrderedDict([('neg', (tensor(0.7895), tensor(0.6998))), ('or', (tensor(0.9258), 0.5)), ('and', (tensor(0.7427), 0.5))])), ('logits', OrderedDict([('neg', (tensor(0.8513, dtype=torch.float64), tensor(0.7322, dtype=torch.float64))), ('or', (tensor(0.6714), 0.5)), ('and', (tensor(0.5132), 0.5))]))])\n",
      "RESIDUALS\n",
      "\n",
      "llama 0.22924417400944108\n",
      "llama_instruct 0.22282578737140224\n",
      "gpt-j 0.1874627389526491\n",
      "gemma 0.21658479639082429\n",
      "gemma_instruct 0.2287662602402427\n",
      "baselines 0.0\n",
      "\n",
      "HEADS\n",
      "\n",
      "llama 0.7124212118812313\n",
      "llama_instruct 0.7018833885522623\n",
      "gpt-j 0.7709687311642905\n",
      "gemma 0.6910831816465205\n",
      "gemma_instruct 0.7031186151227464\n",
      "baselines 0.5\n"
     ]
    }
   ],
   "source": [
    "model = 'llama'\n",
    "llama_data_dict = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logits_logistic_regression_mmp_self_report\")\n",
    "llama_data_dict_heads = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_heads\")\n",
    "\n",
    "model = 'llama_instruct'\n",
    "llama_instruct_data_dict_1 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logits_logistic_regression_mmp\")\n",
    "llama_instruct_data_dict_2 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_self_report\")\n",
    "llama_instruct_data_dict_heads = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_heads\")\n",
    "llama_instruct_data_dict = {**llama_instruct_data_dict_1, **llama_instruct_data_dict_2}\n",
    "\n",
    "model = 'gpt-j'\n",
    "gpt_j_data_dict_1 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logits_logistic_regression_mmp\")\n",
    "gpt_j_data_dict_2 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_self_report\")\n",
    "gpt_j_data_dict_heads = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_heads\")\n",
    "gpt_j_data_dict = {**gpt_j_data_dict_1, **gpt_j_data_dict_2}\n",
    "\n",
    "model = 'gemma'\n",
    "gemma_data_dict_1 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_logits\")\n",
    "gemma_data_dict_2 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_self_report\")\n",
    "gemma_data_dict_heads = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_heads\")\n",
    "gemma_data_dict = {**gemma_data_dict_1, **gemma_data_dict_2}\n",
    "\n",
    "model = 'gemma_instruct'\n",
    "gemma_instruct_data_dict_1 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logits_logistic_regression_mmp\")\n",
    "gemma_instruct_data_dict_2 = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_self_report\")\n",
    "gemma_instruct_data_dict_heads = t.load(PATH / model / 'COHERENCE' / f\"coherence_scoresnegorand_logistic_regression_mmp_heads\")\n",
    "gemma_instruct_data_dict = {**gemma_instruct_data_dict_1, **gemma_instruct_data_dict_2}\n",
    "\n",
    "models = ['llama', 'llama_instruct', 'gpt-j', 'gemma', 'gemma_instruct']\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "key_order = ['logistic_regression', 'mmp', 'self_report', 'logits']\n",
    "\n",
    "def make_ordered_custom(d, order):\n",
    "    \"\"\"\n",
    "    Recursively convert nested dicts into OrderedDicts\n",
    "    following a custom key order.\n",
    "    \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        od = OrderedDict()\n",
    "        for k in order:\n",
    "            if k in d:\n",
    "                od[k] = make_ordered_custom(d[k], order)\n",
    "        # Add any keys not in the custom order at the end\n",
    "        for k in d:\n",
    "            if k not in order:\n",
    "                od[k] = make_ordered_custom(d[k], order)\n",
    "        return od\n",
    "    elif isinstance(d, (list, tuple)):\n",
    "        return type(d)(make_ordered_custom(x, order) if isinstance(x, dict) else x for x in d)\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "# Example: convert your data\n",
    "dicts = [llama_data_dict, llama_instruct_data_dict, gpt_j_data_dict, gemma_data_dict, gemma_instruct_data_dict]\n",
    "dicts_heads = [llama_data_dict_heads, llama_instruct_data_dict_heads, gpt_j_data_dict_heads, gemma_data_dict_heads, gemma_instruct_data_dict_heads]\n",
    "data_ordered = [make_ordered_custom(d, key_order) for d in dicts]\n",
    "data_ordered_heads = [make_ordered_custom(d, key_order) for d in dicts_heads]\n",
    "\n",
    "for heads, full in zip(data_ordered_heads, data_ordered):\n",
    "    for k, v in full.items():\n",
    "        heads.setdefault(k, v)\n",
    "\n",
    "print(\"RESIDUAL\")\n",
    "\n",
    "for model, value in zip(models, data_ordered):\n",
    "    print(model, \"\\n\",  value)\n",
    "print(\"HEADS\")\n",
    "\n",
    "for model, value in zip(models, data_ordered_heads):\n",
    "    print(model, \"\\n\",  value)\n",
    "\n",
    "logreg_data = [x['logistic_regression'] for x in data_ordered]\n",
    "logreg_data_heads = [x['logistic_regression'] for x in data_ordered_heads]\n",
    "\n",
    "def score_coherence_2(data):\n",
    "\n",
    "    diffs = [v[0]-v[1] for v in data.values()]\n",
    "   \n",
    "    return np.mean(diffs) + 0.5\n",
    "\n",
    "model_scores_coherence_2 = []\n",
    "\n",
    "for model in logreg_data:\n",
    "    model_scores_coherence_2.append(score_coherence_2(model))\n",
    "\n",
    "model_scores_coherence_2.append(0.5)\n",
    "models = ['llama', 'llama_instruct', 'gpt-j', 'gemma', 'gemma_instruct', 'baselines']\n",
    "\n",
    "llama_coherence_2 = score_coherence_2(logreg_data[0])\n",
    "llama_instruct_coherence_2 = score_coherence_2(logreg_data[1])\n",
    "gpt_j_coherence_2 = score_coherence_2(logreg_data[2])\n",
    "gemma_coherence_2 = score_coherence_2(logreg_data[3])\n",
    "gemma_instruct_coherence_2 = score_coherence_2(logreg_data[4])\n",
    "print(\"RESIDUALS\\n\")\n",
    "for model, score in zip(models, model_scores_coherence_2):\n",
    "    print(model, score-0.5)\n",
    "\n",
    "print(\"\\nHEADS\\n\")\n",
    "model_scores_coherence_2_heads = []\n",
    "\n",
    "for model in logreg_data_heads:\n",
    "    model_scores_coherence_2_heads.append(score_coherence_2(model))\n",
    "\n",
    "model_scores_coherence_2_heads.append(0.5)\n",
    "models = ['llama', 'llama_instruct', 'gpt-j', 'gemma', 'gemma_instruct', 'baselines']\n",
    "\n",
    "llama_coherence_2_heads = score_coherence_2(logreg_data_heads[0])\n",
    "llama_instruct_coherence_2_heads = score_coherence_2(logreg_data_heads[1])\n",
    "gpt_j_coherence_2_heads = score_coherence_2(logreg_data_heads[2])\n",
    "gemma_coherence_2_heads = score_coherence_2(logreg_data_heads[3])\n",
    "gemma_instruct_coherence_2_heads = score_coherence_2(logreg_data_heads[4])\n",
    "\n",
    "print()\n",
    "print(\"Scores\")\n",
    "print()\n",
    "\n",
    "for model, score in zip(models, model_scores_coherence_2_heads):\n",
    "    print(model, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7bc09",
   "metadata": {},
   "source": [
    "## Coherence (Logic) & Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1918e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama logic [[0.9701087  0.60597826 0.63586957 0.81793478 0.53804348 0.5923913  0.625      0.85326087 0.5298913  0.62228261 0.56793478 0.16576087]\n",
      " [0.94021739 0.52173913 0.54619565 0.55163043 0.50815217 0.5326087  0.52445652 0.53804348 0.50815217 0.52173913 0.44293478 0.48913043]\n",
      " [0.63586957 0.67663043 0.56793478 0.74456522 0.50815217 0.51358696 0.53532609 0.64130435 0.51902174 0.49728261 0.62228261 0.49184783]\n",
      " [0.57608696 0.50543478 0.5625     0.58967391 0.45380435 0.50543478 0.48913043 0.58423913 0.55434783 0.48641304 0.56521739 0.51358696]\n",
      " [0.57065217 0.6576087  0.60507246 0.72644928 0.49818841 0.58514493 0.63586957 0.73731884 0.45018116 0.58152174 0.61594203 0.2201087 ]\n",
      " [0.66032609 0.68931159 0.62137681 0.64855072 0.68025362 0.58152174 0.69474638 0.64402174 0.6576087  0.70471014 0.34963768 0.36865942]\n",
      " [0.39492754 0.47282609 0.47463768 0.56431159 0.33514493 0.48188406 0.44836957 0.59963768 0.33061594 0.39311594 0.7182971  0.36503623]\n",
      " [0.35778986 0.29347826 0.29438406 0.23097826 0.41304348 0.29710145 0.31793478 0.22101449 0.43568841 0.39402174 0.42844203 0.74728261]\n",
      " [0.80902778 0.57291667 0.56597222 0.53472222 0.57291667 0.57986111 0.65625    0.55208333 0.53819444 0.60069444 0.53819444 0.46180556]\n",
      " [0.9375     0.5625     0.4625     0.575      0.4875     0.4625     0.525      0.625      0.45       0.6        0.6875     0.1875    ]\n",
      " [0.825      0.4        0.475      0.4375     0.4375     0.5375     0.45       0.3625     0.4        0.375      0.4125     0.575     ]\n",
      " [0.99791667 0.91041667 0.50416667 0.52083333 0.51041667 0.525      0.51666667 0.83958333 0.55208333 0.52291667 0.82291667 0.48125   ]\n",
      " [0.99375    0.73333333 0.47708333 0.48958333 0.47916667 0.87916667 0.47083333 0.65416667 0.47916667 0.47291667 0.54375    0.48958333]\n",
      " [0.70766129 0.56048387 0.57862903 0.64516129 0.55443548 0.55443548 0.57258065 0.65524194 0.49798387 0.55846774 0.56653226 0.39516129]]\n",
      "llama logicheads [[0.69293478 0.66576087 0.48369565 0.52173913 0.50543478 0.49184783 0.67119565 0.50271739 0.41576087 0.50543478 0.49456522 0.37771739]\n",
      " [0.79619565 0.57065217 0.4701087  0.60869565 0.55706522 0.49728261 0.55978261 0.50543478 0.42663043 0.57336957 0.48641304 0.4375    ]\n",
      " [0.83423913 0.52173913 0.58423913 0.54347826 0.48097826 0.51086957 0.48097826 0.63043478 0.42119565 0.48641304 0.51630435 0.5       ]\n",
      " [0.70652174 0.5298913  0.54076087 0.5        0.50815217 0.51358696 0.51630435 0.61413043 0.4701087  0.51902174 0.5326087  0.47826087]\n",
      " [0.62952899 0.67934783 0.65307971 0.68478261 0.47916667 0.53894928 0.56884058 0.65126812 0.44927536 0.48007246 0.52355072 0.31431159]\n",
      " [0.58514493 0.62228261 0.5625     0.5942029  0.71467391 0.30615942 0.71105072 0.43297101 0.57065217 0.7192029  0.28985507 0.41032609]\n",
      " [0.53985507 0.57608696 0.57518116 0.56793478 0.29619565 0.71014493 0.39492754 0.67753623 0.37771739 0.28713768 0.70833333 0.4067029 ]\n",
      " [0.38496377 0.3134058  0.33514493 0.31884058 0.48278986 0.48007246 0.39583333 0.36141304 0.46557971 0.48007246 0.50634058 0.66938406]\n",
      " [0.72569444 0.50347222 0.54513889 0.51388889 0.55555556 0.48958333 0.5625     0.48611111 0.51388889 0.51388889 0.51736111 0.50694444]\n",
      " [0.8        0.8        0.5875     0.725      0.6        0.575      0.6875     0.55       0.45       0.525      0.5625     0.4125    ]\n",
      " [0.5875     0.425      0.4375     0.4125     0.425      0.625      0.375      0.5375     0.4625     0.4375     0.55       0.4125    ]\n",
      " [0.99583333 0.81875    0.5125     0.50416667 0.50833333 0.59583333 0.50416667 0.5125     0.50416667 0.50416667 0.75208333 0.15416667]\n",
      " [0.98333333 0.49375    0.45625    0.46875    0.33333333 0.575      0.4625     0.48541667 0.44375    0.46041667 0.54583333 0.41666667]\n",
      " [0.61491935 0.53830645 0.54435484 0.56451613 0.49395161 0.53427419 0.52016129 0.55645161 0.46169355 0.48991935 0.52217742 0.42943548]]\n",
      "llama domain [[0.9673913  0.60597826 0.94565217 0.98097826 0.72282609 0.64130435 0.82880435 0.66304348 0.64130435 0.51358696 0.875      0.77717391]\n",
      " [0.72826087 0.6576087  0.69293478 0.67119565 0.67119565 0.57608696 0.6884058  0.52717391 0.7509058  0.48188406 0.6865942  0.54710145]\n",
      " [0.83680556 0.57291667 0.62152778 0.54513889 0.83333333 0.81944444 0.5625     0.48958333 0.50694444 0.50694444 0.61805556 0.61111111]\n",
      " [0.9        0.5625     0.5125     0.675      0.4625     0.5125     0.9        0.9        0.825      0.475      0.5125     0.6       ]\n",
      " [0.99375    0.91041667 0.50416667 0.50416667 0.50416667 0.51041667 0.57291667 0.5125     0.99791667 1.         0.50416667 0.55625   ]\n",
      " [0.71169355 0.56048387 0.61290323 0.64717742 0.60685484 0.625      0.59072581 0.53024194 0.61693548 0.50403226 0.58669355 0.55846774]]\n",
      "llama domainheads [[0.90217391 0.66576087 0.9076087  0.92663043 0.7201087  0.61413043 0.66304348 0.53804348 0.52173913 0.53532609 0.56521739 0.53804348]\n",
      " [0.66847826 0.67934783 0.68115942 0.57699275 0.68931159 0.55253623 0.70199275 0.49818841 0.67119565 0.48822464 0.64311594 0.53532609]\n",
      " [0.70486111 0.50347222 0.52777778 0.51736111 0.67708333 0.72569444 0.49652778 0.51041667 0.49305556 0.50347222 0.55208333 0.5625    ]\n",
      " [0.8        0.8        0.825      0.6625     0.6375     0.55       0.85       0.7875     0.5375     0.5        0.6875     0.5125    ]\n",
      " [0.99583333 0.81875    0.96458333 0.62916667 0.89166667 0.62916667 0.7875     0.5875     0.99375    1.         0.78541667 0.95208333]\n",
      " [0.63508065 0.53830645 0.55443548 0.58467742 0.57258065 0.53225806 0.56451613 0.51209677 0.58467742 0.50403226 0.50604839 0.48185484]]\n",
      "llama_instruct logic [[0.94836957 0.95380435 0.63858696 0.8423913  0.375      0.53804348 0.84782609 0.87771739 0.29619565 0.93478261 0.83967391 0.05978261]\n",
      " [0.94293478 0.62228261 0.53532609 0.50543478 0.49728261 0.51358696 0.5326087  0.61413043 0.55706522 0.44293478 0.42391304 0.26902174]\n",
      " [0.97826087 0.84782609 0.57880435 0.64130435 0.50271739 0.54891304 0.73369565 0.56793478 0.50815217 0.5923913  0.71467391 0.10597826]\n",
      " [0.77173913 0.64402174 0.48913043 0.51086957 0.45652174 0.48097826 0.57065217 0.55163043 0.48913043 0.51086957 0.59782609 0.39945652]\n",
      " [0.55615942 0.72101449 0.52536232 0.67391304 0.46286232 0.48369565 0.66666667 0.7173913  0.40217391 0.63586957 0.65217391 0.18025362]\n",
      " [0.59148551 0.69021739 0.53804348 0.61050725 0.56793478 0.52717391 0.67481884 0.60416667 0.55344203 0.69565217 0.46014493 0.33695652]\n",
      " [0.51358696 0.61413043 0.4990942  0.56521739 0.46376812 0.50996377 0.54528986 0.65126812 0.45380435 0.44293478 0.67028986 0.33695652]\n",
      " [0.30887681 0.1875     0.32336957 0.25815217 0.33333333 0.33152174 0.26177536 0.21648551 0.35960145 0.34873188 0.35597826 0.73550725]\n",
      " [0.79861111 0.56944444 0.54513889 0.59027778 0.55902778 0.52083333 0.65277778 0.55208333 0.49305556 0.57638889 0.52777778 0.40277778]\n",
      " [0.9125     0.8875     0.5125     0.6125     0.4        0.4125     0.8        0.5625     0.425      0.7375     0.5625     0.175     ]\n",
      " [0.9375     0.3125     0.4375     0.5125     0.425      0.4875     0.5375     0.3625     0.375      0.275      0.5125     0.45      ]\n",
      " [0.99791667 0.91666667 0.50625    0.525      0.46458333 0.51875    0.94375    0.95416667 0.50625    0.95625    0.5125     0.22708333]\n",
      " [0.99791667 0.81041667 0.54583333 0.50625    0.5125     0.53333333 0.92291667 0.89375    0.47291667 0.70416667 0.54375    0.32291667]\n",
      " [0.62903226 0.60887097 0.50806452 0.56653226 0.5141129  0.51008065 0.58870968 0.56854839 0.44556452 0.625      0.64112903 0.32258065]]\n",
      "llama_instruct logicheads [[0.9701087  0.95652174 0.70923913 0.87228261 0.50271739 0.63043478 0.89945652 0.88315217 0.43478261 0.70380435 0.71467391 0.14402174]\n",
      " [0.97282609 0.5625     0.69293478 0.66032609 0.55978261 0.625      0.5326087  0.60326087 0.53532609 0.52445652 0.62771739 0.39945652]\n",
      " [0.99184783 0.98641304 0.90217391 0.96195652 0.52717391 0.52445652 0.89402174 0.76358696 0.51358696 0.49728261 0.50815217 0.03804348]\n",
      " [0.87228261 0.5298913  0.5        0.51086957 0.48641304 0.51902174 0.48641304 0.63315217 0.49728261 0.48641304 0.51902174 0.30978261]\n",
      " [0.70471014 0.77626812 0.75       0.77807971 0.51086957 0.58514493 0.67028986 0.69836957 0.4375     0.5307971  0.59601449 0.23007246]\n",
      " [0.59963768 0.63496377 0.61503623 0.61322464 0.71105072 0.33695652 0.69565217 0.47826087 0.55434783 0.71195652 0.3442029  0.37952899]\n",
      " [0.62681159 0.66576087 0.64945652 0.65398551 0.33061594 0.71195652 0.48641304 0.70199275 0.42210145 0.32880435 0.72101449 0.33786232]\n",
      " [0.3125     0.22554348 0.24728261 0.22101449 0.44202899 0.44927536 0.30525362 0.31521739 0.49456522 0.4365942  0.4375     0.76811594]\n",
      " [0.70486111 0.64236111 0.59722222 0.61805556 0.52430556 0.51041667 0.59027778 0.59722222 0.44444444 0.53125    0.53125    0.36805556]\n",
      " [0.8        0.7        0.6        0.6875     0.5375     0.6        0.8375     0.6375     0.375      0.6375     0.5875     0.4       ]\n",
      " [0.8125     0.4375     0.575      0.4625     0.475      0.55       0.475      0.4625     0.4375     0.45       0.525      0.4375    ]\n",
      " [0.99791667 0.98125    0.95833333 0.98333333 0.9        0.49583333 0.97291667 0.60416667 0.62083333 0.975      0.57291667 0.01875   ]\n",
      " [0.99166667 0.94583333 0.77916667 0.89791667 0.725      0.54166667 0.94166667 0.56666667 0.63541667 0.68541667 0.54791667 0.03958333]\n",
      " [0.63508065 0.69556452 0.65120968 0.68346774 0.51209677 0.58064516 0.61491935 0.65322581 0.47580645 0.52419355 0.60282258 0.31048387]]\n",
      "llama_instruct domain [[0.98913043 0.95380435 0.98369565 0.99184783 0.95923913 0.88043478 0.9701087  0.98097826 0.92119565 0.88043478 0.9076087  0.92119565]\n",
      " [0.72826087 0.72101449 0.72644928 0.57518116 0.7173913  0.61594203 0.74094203 0.65036232 0.74456522 0.55615942 0.72735507 0.63949275]\n",
      " [0.83333333 0.56944444 0.58333333 0.52083333 0.82291667 0.87152778 0.63888889 0.57291667 0.56597222 0.50694444 0.61111111 0.61458333]\n",
      " [0.925      0.8875     0.8625     0.725      0.825      0.925      0.9625     0.975      0.8375     0.7        0.85       0.9625    ]\n",
      " [1.         0.91666667 0.92083333 0.98125    0.75416667 0.97291667 0.97291667 0.99375    1.         1.         0.82291667 0.83125   ]\n",
      " [0.69959677 0.60887097 0.62903226 0.62096774 0.66733871 0.71975806 0.61491935 0.69153226 0.63104839 0.55241935 0.58467742 0.56854839]]\n",
      "llama_instruct domainheads [[0.9701087  0.95652174 0.95652174 0.93478261 0.95108696 0.95652174 0.94293478 0.94293478 0.92391304 0.57608696 0.96195652 0.95652174]\n",
      " [0.76992754 0.77626812 0.7807971  0.63949275 0.76811594 0.70018116 0.78713768 0.64583333 0.77264493 0.50181159 0.76177536 0.65307971]\n",
      " [0.81597222 0.64236111 0.64583333 0.62152778 0.82291667 0.89583333 0.63888889 0.66666667 0.64236111 0.52083333 0.65277778 0.62152778]\n",
      " [0.8375     0.7        0.75       0.6625     0.6875     0.6        0.85       0.7875     0.6625     0.6625     0.7875     0.8375    ]\n",
      " [1.         0.98125    0.98541667 0.6625     0.9875     0.52291667 0.96666667 0.98333333 0.98333333 1.         0.89791667 0.65208333]\n",
      " [0.72580645 0.69556452 0.6875     0.62701613 0.71169355 0.67540323 0.69354839 0.66532258 0.69758065 0.52217742 0.72379032 0.67540323]]\n",
      "gpt-j logic [[0.93206522 0.63586957 0.64673913 0.52445652 0.54076087 0.55434783 0.58152174 0.76086957 0.57880435 0.57336957 0.58695652 0.24184783]\n",
      " [0.82880435 0.50543478 0.45108696 0.50271739 0.5        0.50271739 0.51086957 0.42119565 0.48097826 0.49456522 0.44836957 0.57065217]\n",
      " [0.80706522 0.55978261 0.50271739 0.49728261 0.48097826 0.51358696 0.5298913  0.51902174 0.48097826 0.49728261 0.5625     0.41847826]\n",
      " [0.75543478 0.48097826 0.48913043 0.48097826 0.50543478 0.52445652 0.48369565 0.48913043 0.48097826 0.47826087 0.5        0.51358696]\n",
      " [0.56521739 0.67844203 0.53985507 0.71376812 0.44565217 0.44293478 0.6557971  0.70289855 0.25724638 0.65942029 0.66485507 0.14673913]\n",
      " [0.55434783 0.69021739 0.52264493 0.63949275 0.55887681 0.5009058  0.68297101 0.64492754 0.56884058 0.65398551 0.47735507 0.33605072]\n",
      " [0.50452899 0.64492754 0.50181159 0.58695652 0.45108696 0.5298913  0.54076087 0.68297101 0.46467391 0.46105072 0.67119565 0.3115942 ]\n",
      " [0.25       0.16213768 0.19384058 0.1875     0.25       0.22735507 0.25       0.18025362 0.23822464 0.32880435 0.33876812 0.67572464]\n",
      " [0.72222222 0.54166667 0.51041667 0.52430556 0.48263889 0.51736111 0.52430556 0.51041667 0.50347222 0.50694444 0.51041667 0.42708333]\n",
      " [0.6        0.525      0.5625     0.5625     0.4375     0.4875     0.5125     0.5375     0.475      0.475      0.5375     0.4625    ]\n",
      " [0.575      0.3625     0.6        0.55       0.575      0.6        0.5        0.5625     0.6125     0.375      0.5625     0.4625    ]\n",
      " [0.97291667 0.44791667 0.51041667 0.62916667 0.51666667 0.49375    0.50625    0.47291667 0.50208333 0.50416667 0.48958333 0.49791667]\n",
      " [0.92708333 0.51875    0.45625    0.41458333 0.46875    0.46666667 0.4625     0.51041667 0.4625     0.4625     0.51666667 0.46666667]\n",
      " [0.5625     0.52016129 0.54233871 0.55645161 0.4858871  0.55040323 0.53225806 0.56653226 0.52419355 0.5        0.53629032 0.42137097]]\n",
      "gpt-j logicheads [[0.93478261 0.80978261 0.66847826 0.82880435 0.54619565 0.66847826 0.79619565 0.57880435 0.5298913  0.66032609 0.55163043 0.47554348]\n",
      " [0.36956522 0.5        0.51086957 0.49456522 0.52445652 0.48641304 0.51630435 0.28804348 0.50271739 0.52445652 0.32608696 0.77173913]\n",
      " [0.72282609 0.51902174 0.59782609 0.52717391 0.48913043 0.50271739 0.5625     0.51358696 0.48641304 0.54076087 0.56521739 0.42391304]\n",
      " [0.7201087  0.52445652 0.49184783 0.49184783 0.48369565 0.5326087  0.48913043 0.54347826 0.48369565 0.48913043 0.52173913 0.49456522]\n",
      " [0.61775362 0.66213768 0.65851449 0.69293478 0.51539855 0.55797101 0.61865942 0.66666667 0.45742754 0.56702899 0.59148551 0.26630435]\n",
      " [0.61775362 0.63315217 0.62681159 0.62318841 0.69293478 0.41304348 0.69565217 0.51449275 0.58242754 0.70380435 0.35416667 0.39764493]\n",
      " [0.51811594 0.57155797 0.51811594 0.58786232 0.33967391 0.64492754 0.45561594 0.63949275 0.42119565 0.37952899 0.68206522 0.37952899]\n",
      " [0.33333333 0.27173913 0.28351449 0.27264493 0.4057971  0.38315217 0.35416667 0.30344203 0.42028986 0.39673913 0.42753623 0.69021739]\n",
      " [0.65277778 0.51388889 0.53819444 0.52083333 0.50694444 0.48611111 0.59375    0.49652778 0.59722222 0.52430556 0.48958333 0.50694444]\n",
      " [0.575      0.6625     0.5375     0.5625     0.55       0.5375     0.5875     0.5375     0.65       0.575      0.5375     0.4375    ]\n",
      " [0.4625     0.3625     0.5375     0.4125     0.4125     0.55       0.375      0.4875     0.425      0.375      0.55       0.575     ]\n",
      " [0.72916667 0.55625    0.51458333 0.50625    0.50416667 0.68125    0.50416667 0.57291667 0.62291667 0.50416667 0.69166667 0.49583333]\n",
      " [0.47708333 0.48125    0.38333333 0.44166667 0.4625     0.44583333 0.4625     0.37916667 0.31666667 0.4625     0.42916667 0.5375    ]\n",
      " [0.62903226 0.56653226 0.5625     0.61693548 0.50806452 0.54435484 0.53830645 0.58870968 0.48790323 0.52822581 0.54032258 0.41330645]]\n",
      "gpt-j domain [[0.92391304 0.63586957 0.92391304 0.94836957 0.87228261 0.90217391 0.56521739 0.70652174 0.64402174 0.6576087  0.70923913 0.63043478]\n",
      " [0.67753623 0.67844203 0.67028986 0.61594203 0.67300725 0.63768116 0.70199275 0.64673913 0.66757246 0.57518116 0.67119565 0.57065217]\n",
      " [0.84375    0.54166667 0.48958333 0.66666667 0.80555556 0.85763889 0.54513889 0.56944444 0.57986111 0.50347222 0.51388889 0.56597222]\n",
      " [0.825      0.525      0.5625     0.8125     0.6125     0.7        0.775      0.9        0.6625     0.675      0.6375     0.5       ]\n",
      " [0.975      0.44791667 0.51666667 0.57083333 0.46666667 0.48958333 0.5125     0.50416667 0.98958333 0.99375    0.54166667 0.49166667]\n",
      " [0.58467742 0.52016129 0.60887097 0.56653226 0.59072581 0.59475806 0.53225806 0.55241935 0.53225806 0.55040323 0.48790323 0.47983871]]\n",
      "gpt-j domainheads [[0.9375     0.80978261 0.95108696 0.96195652 0.8423913  0.91304348 0.74456522 0.51358696 0.60597826 0.51358696 0.86141304 0.75543478]\n",
      " [0.64673913 0.66213768 0.67844203 0.57699275 0.66032609 0.57699275 0.6576087  0.54528986 0.64855072 0.49275362 0.66576087 0.5326087 ]\n",
      " [0.73958333 0.51388889 0.53472222 0.65972222 0.65277778 0.81944444 0.51388889 0.51041667 0.53472222 0.54861111 0.56597222 0.59027778]\n",
      " [0.7125     0.6625     0.5375     0.525      0.5625     0.5875     0.7375     0.95       0.6125     0.5375     0.6        0.4875    ]\n",
      " [0.6875     0.55625    0.50625    0.50625    0.53125    0.51041667 0.57916667 0.48125    0.98125    1.         0.52708333 0.53958333]\n",
      " [0.65322581 0.56653226 0.65524194 0.64314516 0.59475806 0.66330645 0.57459677 0.53830645 0.54233871 0.52620968 0.59677419 0.56653226]]\n",
      "gemma logic [[0.98913043 0.76086957 0.64945652 0.70923913 0.54619565 0.52717391 0.86956522 0.66576087 0.50543478 0.83695652 0.55434783 0.14130435]\n",
      " [0.99728261 0.53804348 0.50271739 0.54619565 0.57608696 0.60597826 0.56793478 0.5625     0.55978261 0.52717391 0.48913043 0.47826087]\n",
      " [0.94293478 0.60054348 0.60869565 0.7173913  0.48641304 0.67119565 0.56793478 0.70652174 0.57880435 0.53532609 0.65217391 0.13858696]\n",
      " [0.85597826 0.48641304 0.58152174 0.48641304 0.48641304 0.61684783 0.48641304 0.51086957 0.47554348 0.49184783 0.59782609 0.51358696]\n",
      " [0.63496377 0.76630435 0.60326087 0.76721014 0.51902174 0.53623188 0.72101449 0.75815217 0.36050725 0.6259058  0.67300725 0.16576087]\n",
      " [0.60597826 0.66032609 0.54257246 0.60416667 0.59148551 0.48822464 0.68931159 0.57336957 0.50543478 0.69655797 0.44293478 0.31974638]\n",
      " [0.52445652 0.67300725 0.53532609 0.64311594 0.45289855 0.57699275 0.58967391 0.70923913 0.48913043 0.44474638 0.70561594 0.3451087 ]\n",
      " [0.26630435 0.17028986 0.24547101 0.21286232 0.30434783 0.28532609 0.20923913 0.21105072 0.37228261 0.35869565 0.36141304 0.75815217]\n",
      " [0.84027778 0.66319444 0.58680556 0.61111111 0.58333333 0.52430556 0.59722222 0.58333333 0.51041667 0.63541667 0.56597222 0.45486111]\n",
      " [0.95       0.675      0.6875     0.675      0.6625     0.6        0.7875     0.65       0.5125     0.5875     0.5375     0.225     ]\n",
      " [0.9625     0.4875     0.3625     0.375      0.4125     0.5        0.425      0.55       0.4375     0.575      0.5125     0.55      ]\n",
      " [1.         0.9875     0.49791667 0.66458333 0.50208333 0.50208333 0.51041667 0.93125    0.50208333 0.51875    0.49791667 0.00416667]\n",
      " [1.         0.73333333 0.5375     0.54375    0.53958333 0.5375     0.53541667 0.55416667 0.4625     0.51875    0.5375     0.25833333]\n",
      " [0.69758065 0.67137097 0.58064516 0.61895161 0.52016129 0.54233871 0.63508065 0.63709677 0.49798387 0.55443548 0.59879032 0.30241935]]\n",
      "gemma logicheads [[0.98369565 0.96467391 0.69293478 0.89130435 0.83423913 0.52445652 0.97826087 0.79619565 0.5326087  0.95108696 0.58695652 0.03804348]\n",
      " [0.99184783 0.7826087  0.85869565 0.82608696 0.52445652 0.54619565 0.57608696 0.86956522 0.38315217 0.52445652 0.72554348 0.2826087 ]\n",
      " [0.9673913  0.50271739 0.90217391 0.59782609 0.49184783 0.76902174 0.49184783 0.94565217 0.5        0.49184783 0.79347826 0.24184783]\n",
      " [0.83152174 0.48641304 0.57065217 0.48641304 0.48641304 0.60869565 0.48641304 0.50543478 0.48913043 0.48641304 0.50815217 0.51358696]\n",
      " [0.67844203 0.79891304 0.73188406 0.78713768 0.54438406 0.5932971  0.74637681 0.73369565 0.42210145 0.60597826 0.63586957 0.17934783]\n",
      " [0.58876812 0.65851449 0.60688406 0.63768116 0.70289855 0.41485507 0.69021739 0.55525362 0.49365942 0.72101449 0.38586957 0.34782609]\n",
      " [0.61865942 0.67753623 0.61413043 0.67663043 0.35869565 0.68478261 0.57155797 0.71105072 0.46286232 0.40217391 0.7182971  0.32971014]\n",
      " [0.30163043 0.20742754 0.21648551 0.21105072 0.40942029 0.36141304 0.24547101 0.25362319 0.47192029 0.38496377 0.39039855 0.79438406]\n",
      " [0.85763889 0.65277778 0.72569444 0.69097222 0.60416667 0.55902778 0.64583333 0.60416667 0.49305556 0.5625     0.55555556 0.36111111]\n",
      " [0.975      0.9375     0.75       0.9375     0.5        0.5875     0.8875     0.675      0.2625     0.5625     0.5375     0.225     ]\n",
      " [0.9625     0.5375     0.525      0.625      0.425      0.5625     0.55       0.5375     0.4125     0.4375     0.5625     0.475     ]\n",
      " [1.         0.97916667 0.53333333 0.95       0.50416667 0.49583333 0.99166667 0.49583333 0.475      0.50416667 0.51458333 0.03125   ]\n",
      " [1.         0.5625     0.51041667 0.5625     0.4625     0.56458333 0.90833333 0.5375     0.7125     0.4625     0.54375    0.25625   ]\n",
      " [0.72177419 0.68951613 0.63709677 0.68548387 0.5625     0.52822581 0.6391129  0.65725806 0.47983871 0.58266129 0.58266129 0.31854839]]\n",
      "gemma domain [[1.         0.76086957 0.99184783 1.         0.78804348 0.89130435 0.91032609 0.82336957 0.94293478 0.51630435 0.92663043 0.85597826]\n",
      " [0.77173913 0.76630435 0.76630435 0.68478261 0.77445652 0.59782609 0.78351449 0.49728261 0.77717391 0.49003623 0.77083333 0.6548913 ]\n",
      " [0.87847222 0.66319444 0.68402778 0.52083333 0.87152778 0.89583333 0.63194444 0.50694444 0.59375    0.50694444 0.67013889 0.58333333]\n",
      " [0.9625     0.675      0.7625     0.7625     0.7125     0.6125     1.         0.9        0.8125     0.5875     0.7875     0.5625    ]\n",
      " [1.         0.9875     1.         0.50416667 0.78958333 0.50208333 0.97291667 0.50833333 1.         1.         0.52083333 0.50416667]\n",
      " [0.68346774 0.67137097 0.69556452 0.75       0.69758065 0.5766129  0.69153226 0.51814516 0.66532258 0.49798387 0.73991935 0.74798387]]\n",
      "gemma domainheads [[1.         0.96467391 0.99184783 0.99728261 0.94293478 0.75       0.9375     0.95923913 0.94836957 0.67391304 0.98369565 0.95652174]\n",
      " [0.7798913  0.79891304 0.79891304 0.6576087  0.79710145 0.68115942 0.79347826 0.51358696 0.78985507 0.59692029 0.78804348 0.6576087 ]\n",
      " [0.86805556 0.65277778 0.72916667 0.61111111 0.83680556 0.92708333 0.61111111 0.51736111 0.57638889 0.55208333 0.77083333 0.67361111]\n",
      " [0.95       0.9375     0.925      0.925      0.8125     0.575      0.9875     0.9875     0.9375     0.75       0.9        0.8       ]\n",
      " [1.         0.97916667 1.         0.50416667 0.98958333 0.50416667 0.97291667 0.51041667 1.         1.         1.         0.775     ]\n",
      " [0.74798387 0.68951613 0.69354839 0.74596774 0.72177419 0.70766129 0.69354839 0.63306452 0.68951613 0.58266129 0.76814516 0.75604839]]\n",
      "gemma_instruct logic [[0.98913043 0.91304348 0.74728261 0.8125     0.61956522 0.69021739 0.91032609 0.9076087  0.60326087 0.625      0.85597826 0.05163043]\n",
      " [0.95923913 0.54891304 0.61684783 0.6548913  0.57880435 0.56521739 0.50815217 0.66847826 0.60597826 0.70108696 0.57065217 0.34782609]\n",
      " [0.96467391 0.76902174 0.64402174 0.79619565 0.49184783 0.83967391 0.67391304 0.82880435 0.52445652 0.63043478 0.80163043 0.05434783]\n",
      " [0.85597826 0.46195652 0.43478261 0.47282609 0.54619565 0.55163043 0.49456522 0.45923913 0.5        0.5326087  0.45923913 0.39673913]\n",
      " [0.65217391 0.80525362 0.61413043 0.75271739 0.53894928 0.55978261 0.75271739 0.74818841 0.40398551 0.68297101 0.65126812 0.17119565]\n",
      " [0.61865942 0.66847826 0.54800725 0.61684783 0.56431159 0.48550725 0.69112319 0.58786232 0.50996377 0.71648551 0.44746377 0.32518116]\n",
      " [0.52445652 0.66757246 0.58967391 0.66123188 0.49728261 0.60688406 0.59692029 0.69202899 0.4990942  0.47373188 0.69112319 0.35054348]\n",
      " [0.25996377 0.17572464 0.27083333 0.2201087  0.30706522 0.32789855 0.20652174 0.23460145 0.39039855 0.32789855 0.35144928 0.7817029 ]\n",
      " [0.89930556 0.72916667 0.65972222 0.625      0.57638889 0.54861111 0.78819444 0.75       0.50347222 0.55902778 0.74305556 0.24305556]\n",
      " [0.975      0.925      0.575      0.5875     0.625      0.475      0.825      0.6875     0.5875     0.55       0.55       0.175     ]\n",
      " [0.9875     0.575      0.4625     0.5875     0.025      0.475      0.375      0.6125     0.425      0.5125     0.6375     0.2375    ]\n",
      " [0.99583333 0.50208333 0.44375    0.51041667 0.38541667 0.55625    0.9875     0.21041667 0.50208333 0.63958333 0.09583333 0.48541667]\n",
      " [1.         0.52291667 0.55208333 0.5375     0.6375     0.9875     0.94583333 0.25416667 0.48333333 0.52708333 0.03541667 0.34791667]\n",
      " [0.78427419 0.76814516 0.61491935 0.66935484 0.60887097 0.60282258 0.7016129  0.70766129 0.5141129  0.64314516 0.65927419 0.25604839]]\n",
      "gemma_instruct logicheads [[0.95108696 0.99456522 0.94021739 0.91847826 0.7173913  0.85597826 0.99728261 0.86956522 0.51086957 0.89130435 0.49184783 0.32880435]\n",
      " [0.95380435 0.85054348 0.50543478 0.64673913 0.51358696 0.49456522 0.55978261 0.70923913 0.5326087  0.51086957 0.47554348 0.28532609]\n",
      " [0.99184783 0.97554348 0.91847826 0.9701087  0.48641304 0.91032609 0.50815217 0.94021739 0.49184783 0.51086957 0.58967391 0.17119565]\n",
      " [0.89673913 0.51630435 0.52445652 0.69293478 0.51630435 0.55706522 0.68206522 0.66032609 0.49184783 0.50815217 0.51358696 0.48913043]\n",
      " [0.71014493 0.81612319 0.76268116 0.82518116 0.5615942  0.60326087 0.76992754 0.7826087  0.43025362 0.60144928 0.62318841 0.1567029 ]\n",
      " [0.61594203 0.64945652 0.61865942 0.63586957 0.70742754 0.40036232 0.70380435 0.58061594 0.54800725 0.72373188 0.37771739 0.34148551]\n",
      " [0.61141304 0.68206522 0.66123188 0.65851449 0.38405797 0.69565217 0.59782609 0.70652174 0.44655797 0.38405797 0.73460145 0.32065217]\n",
      " [0.25543478 0.17028986 0.21286232 0.18297101 0.37952899 0.36684783 0.2173913  0.20833333 0.4375     0.37771739 0.39673913 0.8125    ]\n",
      " [0.88888889 0.875      0.75347222 0.84027778 0.51388889 0.5625     0.76041667 0.75347222 0.52083333 0.52777778 0.52430556 0.21875   ]\n",
      " [0.9125     0.9625     0.95       0.95       0.6        0.55       0.625      0.8625     0.4125     0.45       0.725      0.025     ]\n",
      " [0.85       0.7        0.425      0.7125     0.45       0.5625     0.45       0.75       0.475      0.425      0.55       0.525     ]\n",
      " [0.99166667 0.52083333 0.60208333 0.88541667 0.50625    0.45625    0.98958333 0.49791667 0.05416667 0.59166667 0.62083333 0.50416667]\n",
      " [1.         0.85416667 0.84375    0.72916667 0.4625     0.66875    0.8375     0.96458333 0.78333333 0.4625     0.56875    0.43125   ]\n",
      " [0.80241935 0.78225806 0.67540323 0.71774194 0.55040323 0.64112903 0.70967742 0.75604839 0.52016129 0.6391129  0.60483871 0.26814516]]\n",
      "gemma_instruct domain [[1.         0.91304348 0.99456522 0.99728261 0.94293478 0.86684783 0.97282609 0.82880435 0.8451087  0.63586957 0.90217391 0.93206522]\n",
      " [0.80072464 0.80525362 0.79528986 0.7201087  0.78894928 0.69112319 0.79981884 0.52083333 0.80344203 0.5307971  0.7798913  0.68478261]\n",
      " [0.92013889 0.72916667 0.77777778 0.85069444 0.90277778 0.92013889 0.73263889 0.50694444 0.67361111 0.5        0.77777778 0.78472222]\n",
      " [0.9875     0.925      0.9375     0.9375     0.875      0.925      0.9875     0.9        0.9375     0.7375     0.95       0.925     ]\n",
      " [0.99583333 0.50208333 0.62916667 0.99166667 0.52083333 0.80416667 0.49791667 0.50833333 0.99583333 0.99791667 0.97083333 0.49583333]\n",
      " [0.74395161 0.76814516 0.7641129  0.77419355 0.76612903 0.62903226 0.75201613 0.58467742 0.72782258 0.56048387 0.78830645 0.75604839]]\n",
      "gemma_instruct domainheads [[0.99456522 0.99456522 1.         1.         0.97826087 0.99456522 0.94293478 0.99456522 0.97282609 0.94021739 0.99728261 0.98097826]\n",
      " [0.80615942 0.81612319 0.81702899 0.73097826 0.80978261 0.78351449 0.82427536 0.79257246 0.82336957 0.7173913  0.80072464 0.71105072]\n",
      " [0.92361111 0.875      0.86111111 0.82291667 0.91666667 0.93402778 0.88194444 0.89930556 0.87152778 0.55902778 0.88541667 0.82986111]\n",
      " [0.925      0.9625     0.95       0.9375     0.975      0.975      0.975      1.         0.95       0.95       0.95       0.9       ]\n",
      " [0.99166667 0.52083333 0.98958333 0.98958333 0.59583333 0.71041667 0.57083333 0.54166667 0.99375    0.99583333 0.98333333 0.96666667]\n",
      " [0.8266129  0.78225806 0.78427419 0.79032258 0.78830645 0.76209677 0.76008065 0.80846774 0.76814516 0.74395161 0.83064516 0.82459677]]\n"
     ]
    }
   ],
   "source": [
    "models = ['llama', 'llama_instruct', 'gpt-j', 'gemma', 'gemma_instruct']\n",
    "unif_types = ['logic', 'logicheads', 'domain', 'domainheads']\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "# Container for all matrices\n",
    "matrices = {}\n",
    "\n",
    "for model in models:\n",
    "    matrices[model] = {}\n",
    "    for UNIF_TYPE in unif_types:\n",
    "        if UNIF_TYPE in ['domain', 'domainheads']:\n",
    "            x_ticks = ['All_Datasets', 'Common_Claims', 'CC+Cities', 'cities', 'CC+Companies', 'companies', \n",
    "                       'CC+Sp_en', 'Sp_en', 'CC+Larger_Than', 'Larger_than', 'CC+Counterfact', 'Counterfact']\n",
    "            y_ticks = ['cities', 'Common_Claims', 'companies', 'Sp_En', 'Larger_Than', 'Counterfact']\n",
    "\n",
    "        elif UNIF_TYPE in ['logic', 'logicheads']:\n",
    "            x_ticks = ['All_Datasets', 'Common_Claims', 'CC+C+D+N', 'CC+C+D', 'CC+C+N', 'CC+D+N', \n",
    "                       'CC+C', 'CC+D', 'CC+N', 'Only_Conj', 'Only_Disj', 'Only_neg']\n",
    "            y_ticks = ['cities', 'neg_cities', 'conj_cities', 'disj_cities', 'Common_Claims', 'Conj_Common_Claims',\n",
    "                       'Disj_Common_Claims', 'Neg_Common_Claims', 'companies', 'Sp_en', 'Neg_Sp_En', \n",
    "                       'Larger_Than', 'Smaller_Than', 'Counterfact']\n",
    "        else:\n",
    "            raise ValueError(\"UNIF_TYPE not recognized\")\n",
    "\n",
    "        # Load the data\n",
    "        data_sweep = t.load(PATH / model / 'UNIFORMITY' / f\"uniformity_uniformity{UNIF_TYPE}\")\n",
    "        data_array = np.array([[data_sweep[row][col][0] for col in sorted(data_sweep[row])] \n",
    "                               for row in sorted(data_sweep)]).T\n",
    "\n",
    "        # Store the full matrix\n",
    "        matrices[model][UNIF_TYPE] = data_array\n",
    "\n",
    "        print(model, UNIF_TYPE, data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7434ec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== LOGIC ===========\n",
      "\n",
      "RESIDUALS\n",
      "llama \n",
      "general 0.4031002976693211 all_datasets 0.5318043842582401\n",
      "llama_instruct \n",
      "general 0.36961754379750555 all_datasets 0.5648162507072334\n",
      "gpt_j \n",
      "general 0.38601121711885966 all_datasets 0.4877322158634759\n",
      "gemma \n",
      "general 0.3874629960165311 all_datasets 0.5869364175167491\n",
      "gemma_instruct \n",
      "general 0.36699448959868824 all_datasets 0.6014197270555243\n",
      "\n",
      "HEADS\n",
      "llama \n",
      "general 0.4074889749893479 all_datasets 0.5432705314110398\n",
      "llama_instruct \n",
      "general 0.39484853083649335 all_datasets 0.5911866524633791\n",
      "gpt_j \n",
      "general 0.4123788788411172 all_datasets 0.4440703676487687\n",
      "gemma \n",
      "general 0.39009399405761847 all_datasets 0.6161804678767203\n",
      "gemma_instruct \n",
      "general 0.4020422087115511 all_datasets 0.6160866238689355\n",
      "\n",
      "=========== DOMAIN ===========\n",
      "\n",
      "RESIDUALS\n",
      "llama \n",
      "general 0.5060460867301433 all_datasets 0.747667177367418\n",
      "llama_instruct \n",
      "general 0.6273841045124473 all_datasets 0.7440517758617988\n",
      "gpt_j \n",
      "general 0.5057649197070717 all_datasets 0.6697445155165004\n",
      "gemma \n",
      "general 0.5757640961829853 all_datasets 0.7630370675660534\n",
      "gemma_instruct \n",
      "general 0.6414518582301165 all_datasets 0.8071401826715009\n",
      "\n",
      "HEADS\n",
      "llama \n",
      "general 0.5060198841579664 all_datasets 0.6547391649037266\n",
      "llama_instruct \n",
      "general 0.6266108956015205 all_datasets 0.7532077368133785\n",
      "gpt_j \n",
      "general 0.502823627369835 all_datasets 0.6311161522850477\n",
      "gemma \n",
      "general 0.6395400776183372 all_datasets 0.7905160691013419\n",
      "gemma_instruct \n",
      "general 0.7429446716917125 all_datasets 0.8382879537229657\n"
     ]
    }
   ],
   "source": [
    "def score_uniformity(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = np.mean(matrix) - np.std(matrix)\n",
    "    \n",
    "    # First column score\n",
    "    first_col = matrix[:, 0]\n",
    "    first_col_score = np.mean(first_col) - np.std(first_col)\n",
    "    \n",
    "    return overall_score, first_col_score\n",
    "\n",
    "print()\n",
    "print(\"=========== LOGIC ===========\")\n",
    "print()\n",
    "llama_coherence_1 = score_uniformity(matrices['llama']['logic']) \n",
    "llama_instruct_coherence_1 = score_uniformity(matrices['llama_instruct']['logic']) \n",
    "gpt_j_coherence_1 = score_uniformity(matrices['gpt-j']['logic']) \n",
    "gemma_coherence_1 = score_uniformity(matrices['gemma']['logic']) \n",
    "gemma_instruct_coherence_1 = score_uniformity(matrices['gemma_instruct']['logic']) \n",
    "llama_coherence_1_heads = score_uniformity(matrices['llama']['logicheads']) \n",
    "llama_instruct_coherence_1_heads = score_uniformity(matrices['llama_instruct']['logicheads']) \n",
    "gpt_j_coherence_1_heads = score_uniformity(matrices['gpt-j']['logicheads']) \n",
    "gemma_coherence_1_heads = score_uniformity(matrices['gemma']['logicheads']) \n",
    "gemma_instruct_coherence_1_heads = score_uniformity(matrices['gemma_instruct']['logicheads']) \n",
    "\n",
    "\n",
    "coherence_scores = {\n",
    "    \"llama\": llama_coherence_1,\n",
    "    \"llama_instruct\": llama_instruct_coherence_1,\n",
    "    \"gpt_j\": gpt_j_coherence_1,\n",
    "    \"gemma\": gemma_coherence_1,\n",
    "    \"gemma_instruct\": gemma_instruct_coherence_1\n",
    "}\n",
    "\n",
    "coherence_scores_heads = {\n",
    "    \"llama\": llama_coherence_1_heads,\n",
    "    \"llama_instruct\": llama_instruct_coherence_1_heads,\n",
    "    \"gpt_j\": gpt_j_coherence_1_heads,\n",
    "    \"gemma\": gemma_coherence_1_heads,\n",
    "    \"gemma_instruct\": gemma_instruct_coherence_1_heads\n",
    "}\n",
    "\n",
    "print(\"RESIDUALS\")\n",
    "for model, score in coherence_scores.items():\n",
    "    print(model, \"\\ngeneral\", score[0], \"all_datasets\", score[1])\n",
    "\n",
    "print()\n",
    "print(\"HEADS\")\n",
    "for model, score in coherence_scores_heads.items():\n",
    "    print(model, \"\\ngeneral\", score[0], \"all_datasets\", score[1])\n",
    "\n",
    "print()\n",
    "print(\"=========== DOMAIN ===========\")\n",
    "print()\n",
    "\n",
    "llama_domain = score_uniformity(matrices['llama']['domain']) \n",
    "llama_instruct_domain = score_uniformity(matrices['llama_instruct']['domain']) \n",
    "gpt_j_domain = score_uniformity(matrices['gpt-j']['domain']) \n",
    "gemma_domain = score_uniformity(matrices['gemma']['domain']) \n",
    "gemma_instruct_domain = score_uniformity(matrices['gemma_instruct']['domain']) \n",
    "llama_domain_heads = score_uniformity(matrices['llama']['domainheads']) \n",
    "llama_instruct_domain_heads = score_uniformity(matrices['llama_instruct']['domainheads']) \n",
    "gpt_j_domain_heads = score_uniformity(matrices['gpt-j']['domainheads']) \n",
    "gemma_domain_heads = score_uniformity(matrices['gemma']['domainheads']) \n",
    "gemma_instruct_domain_heads = score_uniformity(matrices['gemma_instruct']['domainheads']) \n",
    "\n",
    "\n",
    "domain_scores = {\n",
    "    \"llama\": llama_domain,\n",
    "    \"llama_instruct\": llama_instruct_domain,\n",
    "    \"gpt_j\": gpt_j_domain,\n",
    "    \"gemma\": gemma_domain,\n",
    "    \"gemma_instruct\": gemma_instruct_domain\n",
    "}\n",
    "\n",
    "domain_scores_heads = {\n",
    "    \"llama\": llama_domain_heads,\n",
    "    \"llama_instruct\": llama_instruct_domain_heads,\n",
    "    \"gpt_j\": gpt_j_domain_heads,\n",
    "    \"gemma\": gemma_domain_heads,\n",
    "    \"gemma_instruct\": gemma_instruct_domain_heads\n",
    "}\n",
    "\n",
    "print(\"RESIDUALS\")\n",
    "for model, score in domain_scores.items():\n",
    "    print(model, \"\\ngeneral\", score[0], \"all_datasets\", score[1])\n",
    "\n",
    "print()\n",
    "print(\"HEADS\")\n",
    "for model, score in domain_scores_heads.items():\n",
    "    print(model, \"\\ngeneral\", score[0], \"all_datasets\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beliefs_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
